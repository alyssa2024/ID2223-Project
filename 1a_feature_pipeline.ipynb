{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d86f0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import hopsworks\n",
    "import hsfs\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from hsfs.embedding import EmbeddingIndex\n",
    "\n",
    "from functions.zotero_parser import ZoteroCSVParser\n",
    "from functions.PDF_extractor import PDFExtractor\n",
    "import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33fdd29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "ZOTERO_CSV_PATH = \"PCG_latest.csv\"\n",
    "\n",
    "# Hopsworks\n",
    "FEATURE_GROUP_PAPER = \"paper_metadata_fg_2\"\n",
    "FEATURE_GROUP_CHUNK = \"paper_chunk_fg_2\"\n",
    "\n",
    "# Embedding\n",
    "from config import EMBEDDING_MODEL_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93deab26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import Optional, Dict, Any\n",
    "\n",
    "def sanitize_paper_metadata(paper: Dict[str, Any]) -> Optional[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Defensive metadata sanitation.\n",
    "    Returns None if the paper is considered invalid.\n",
    "    \"\"\"\n",
    "\n",
    "    # ---- 1. Mandatory Field Validation ----\n",
    "    title = paper.get(\"title\", \"\").strip()\n",
    "    if not title:\n",
    "        return None  # Equivalent to RDF version: discard if title is missing\n",
    "\n",
    "    paper[\"title\"] = title\n",
    "\n",
    "    # ---- 2. Year Repair (Reusing regex logic from RDF) ----\n",
    "    year = paper.get(\"year\")\n",
    "    if year is None:\n",
    "        # Attempt to recover year from other metadata fields\n",
    "        for field in (\"url\", \"abstract\"):\n",
    "            text = paper.get(field, \"\")\n",
    "            match = re.search(r\"(19|20)\\d{2}\", text)\n",
    "            if match:\n",
    "                paper[\"year\"] = int(match.group())\n",
    "                break\n",
    "\n",
    "    # ---- 3. Authors Fallback ----\n",
    "    authors = paper.get(\"authors\", \"\").strip()\n",
    "    if not authors or authors.lower() == \"nan\":\n",
    "        paper[\"authors\"] = \"Unknown\"\n",
    "\n",
    "    # ---- 4. Abstract Normalization ----\n",
    "    abstract = paper.get(\"abstract\", \"\").strip()\n",
    "    if abstract.lower() in {\"nan\", \"none\"}:\n",
    "        paper[\"abstract\"] = \"\"\n",
    "\n",
    "    # ---- 5. Attachments Handling ----\n",
    "    # Preserve original state but ensure the value is a string\n",
    "    attachments = paper.get(\"file_attachments\")\n",
    "    paper[\"file_attachments\"] = str(attachments) if attachments is not None else \"\"\n",
    "\n",
    "    return paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b008d32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed 17 papers.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'paper_id': 'CLGNKPIJ',\n",
       "  'title': 'Synthesis of Normal Heart Sounds Using Generative Adversarial Networks and Empirical Wavelet Transform',\n",
       "  'authors': 'Narváez, Pedro; Percybrooks, Winston S.',\n",
       "  'year': 2020,\n",
       "  'abstract': 'Currently, there are many works in the literature focused on the analysis of heart sounds, speciﬁcally on the development of intelligent systems for the classiﬁcation of normal and abnormal heart sounds. However, the available heart sound databases are not yet large enough to train generalized machine learning models. Therefore, there is interest in the development of algorithms capable of generating heart sounds that could augment current databases. In this article, we propose a model based on generative adversary networks (GANs) to generate normal synthetic heart sounds. Additionally, a denoising algorithm is implemented using the empirical wavelet transform (EWT), allowing a decrease in the number of epochs and the computational cost that the GAN model requires. A distortion metric (mel–cepstral distortion) was used to objectively assess the quality of synthetic heart sounds. The proposed method was favorably compared with a mathematical model that is based on the morphology of the phonocardiography (PCG) signal published as the state of the art. Additionally, diﬀerent heart sound classiﬁcation models proposed as state-of-the-art were also used to test the performance of such models when the GAN-generated synthetic signals were used as test dataset. In this experiment, good accuracy results were obtained with most of the implemented models, suggesting that the GAN-generated sounds correctly capture the characteristics of natural heart sounds.',\n",
       "  'item_type': 'journalArticle',\n",
       "  'file_attachments': 'C:\\\\Users\\\\alyssa\\\\Zotero\\\\storage\\\\J6CFPY56\\\\Narváez和Percybrooks - 2020 - Synthesis of Normal Heart Sounds Using Generative Adversarial Networks and Empirical Wavelet Transfo.pdf',\n",
       "  'url': 'https://www.mdpi.com/2076-3417/10/19/7003'},\n",
       " {'paper_id': 'R7JAHFY6',\n",
       "  'title': 'ECG Generation Based on Denoising Diffusion Probabilistic Models',\n",
       "  'authors': 'Wang, Zhongyu; Ma, Caiyun; Zhao, Minghui; Zhang, Shuo; Li, Jianqing; Liu, Chengyu',\n",
       "  'year': 2024,\n",
       "  'abstract': 'Arrhythmia diseases seriously damage people’s life and health, and identifying abnormal points in ECG signals by deep neural networks is an effective method for detecting arrhythmias. However, their accuracy is often limited by the biased data distribution of the training set, and a large number of labeled ECG signals are usually harder to obtain. Therefore, this paper proposes to synthesize virtual heart beat data by denoising diffusion probability model (DDPM) based on the MIT-BIH arrhythmia database to complement the real data. Three different methods for generating heartbeat signals are also used, which are (i) generating heartbeat signals directly, (ii) generating time-frequency maps of heartbeats and transforming them into heartbeat signals, and (iii) generating sub-signals of heartbeats and fusing them into complete heartbeat signals. Regarding the evaluation of the synthesized signals, we compare the advantages and disadvantages of the three heartbeat generation methods by four metrics: DTW, PCC, ED and KLD. The experimental results showed that the optimum values of 4.37, 17.09, 0.972 and 0.0094 were obtained for ED, DTW of method (i) and PCC, KLD of method (iii), respectively.',\n",
       "  'item_type': 'conferencePaper',\n",
       "  'file_attachments': 'C:\\\\Users\\\\alyssa\\\\Zotero\\\\storage\\\\JZPFTQVT\\\\Wang 等 - 2024 - ECG Generation Based on Denoising Diffusion Probabilistic Models.pdf',\n",
       "  'url': 'https://www.cinc.org/archives/2024/pdf/CinC2024-027.pdf'}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = ZoteroCSVParser(ZOTERO_CSV_PATH)\n",
    "raw_papers = parser.parse()\n",
    "\n",
    "papers_df = []\n",
    "for paper in raw_papers:\n",
    "    fixed = sanitize_paper_metadata(paper)\n",
    "    if fixed is not None:\n",
    "        papers_df.append(fixed)\n",
    "\n",
    "print(f\"Parsed {len(papers_df)} papers.\")\n",
    "papers_df[:2]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c2ffa72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-11 18:04:15,827 INFO: Use pytorch device_name: cpu\n",
      "2026-01-11 18:04:15,828 INFO: Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "Loaded embedding model: all-MiniLM-L6-v2\n",
      "Embedding dimension: 384\n"
     ]
    }
   ],
   "source": [
    "# === Cell 5: Generate Embeddings for Metadata and Full Text ===\n",
    "\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from hsfs import embedding\n",
    "\n",
    "# -------------------------\n",
    "# 1. Load embedding model\n",
    "# -------------------------\n",
    "\n",
    "model = SentenceTransformer(EMBEDDING_MODEL_NAME)\n",
    "embedding_dim = model.get_sentence_embedding_dimension()\n",
    "\n",
    "print(f\"Loaded embedding model: {EMBEDDING_MODEL_NAME}\")\n",
    "print(f\"Embedding dimension: {embedding_dim}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d127fe4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-11 18:04:18,496 INFO: Initializing external client\n",
      "2026-01-11 18:04:18,497 INFO: Base URL: https://c.app.hopsworks.ai:443\n",
      "2026-01-11 18:04:20,077 INFO: Python Engine initialized.\n",
      "\n",
      "Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1286333\n"
     ]
    }
   ],
   "source": [
    "from config import HOPSWORKS_API_KEY\n",
    "# project = hopsworks.login()\n",
    "\n",
    "project = hopsworks.login(\n",
    "        # project=HOPSWORKS_PROJECT,\n",
    "        api_key_value=HOPSWORKS_API_KEY\n",
    "    )\n",
    "fs = project.get_feature_store()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a66dedff",
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_fg = fs.get_feature_group(\n",
    "    name=FEATURE_GROUP_PAPER,\n",
    "    version=2\n",
    ")\n",
    "\n",
    "chunk_fg = fs.get_feature_group(\n",
    "    name=FEATURE_GROUP_CHUNK,\n",
    "    version=2\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "888d2e28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (0.86s) \n",
      "Papers already in Feature Store: 15\n"
     ]
    }
   ],
   "source": [
    "existing_papers_df = paper_fg.read(online=False)[[\"paper_id\"]]\n",
    "\n",
    "existing_paper_ids = set(existing_papers_df[\"paper_id\"].astype(str))\n",
    "\n",
    "print(f\"Papers already in Feature Store: {len(existing_paper_ids)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ddbfc32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New papers detected: 2\n"
     ]
    }
   ],
   "source": [
    "new_papers = []\n",
    "\n",
    "for paper in papers_df:   \n",
    "    paper_id = str(paper[\"paper_id\"])\n",
    "    if paper_id not in existing_paper_ids:\n",
    "        new_papers.append(paper)\n",
    "\n",
    "print(f\"New papers detected: {len(new_papers)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ac692cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import urllib.parse\n",
    "from typing import List, Optional\n",
    "\n",
    "from config import MIN_FULLTEXT_LEN, CHUNK_SIZE, CHUNK_OVERLAP\n",
    "\n",
    "def extract_abstract_from_text(text: str) -> Optional[str]:\n",
    "    normalized = text.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")\n",
    "\n",
    "    stop_markers = (\n",
    "        r\"keywords|index\\s*terms|subject[s]?|introduction|background|materials\\s+and\\s+methods|\"\n",
    "        r\"methods|results|conclusions|references|acknowledg(e)?ments|1\\.|i\\.|ii\\.|iii\\.\"\n",
    "        r\"|Keywords|Introduction|Background|Methods|Results|Conclusion|References\"\n",
    "    )\n",
    "    start_markers = r\"abstract|summary|Abstract|Summary\"\n",
    "\n",
    "    pattern = rf\"(?is)\\b(?:{start_markers})\\b\\s*[:\\.\\-]?\\s*(.+?)(?=\\n\\s*(?:{stop_markers})\\b|\\n\\n\\s*[A-Z][A-Za-z ]+\\b|\\Z)\"\n",
    "    match = re.search(pattern, normalized)\n",
    "    if match:\n",
    "        abstract = re.sub(r\"\\s+\", \" \", match.group(1).strip())\n",
    "        if 50 <= len(abstract) <= 5000 and re.search(r\"[a-z]\", abstract, re.I):\n",
    "            return abstract\n",
    "\n",
    "    lines = normalized.split(\"\\n\")\n",
    "    abstract_started = False\n",
    "    buffer: list[str] = []\n",
    "\n",
    "    for line in lines:\n",
    "        line_stripped = line.strip()\n",
    "\n",
    "        if not abstract_started:\n",
    "            if re.match(\n",
    "                r\"(?i)^(abstract|summary)\\b\\s*[:\\-\\.]?\\s*$\",\n",
    "                line_stripped,\n",
    "            ) or re.match(\n",
    "                r\"(?i)^(abstract|summary)\\b\\s*[:\\-\\.]?\",\n",
    "                line_stripped,\n",
    "            ):\n",
    "                after = re.sub(\n",
    "                    r\"(?i)^(abstract|summary)\\b\\s*[:\\-\\.]?\\s*\",\n",
    "                    \"\",\n",
    "                    line_stripped,\n",
    "                )\n",
    "                if after:\n",
    "                    buffer.append(after)\n",
    "                abstract_started = True\n",
    "            continue\n",
    "        else:\n",
    "            if re.match(rf\"(?i)^\\s*(?:{stop_markers})\\b\", line_stripped):\n",
    "                break\n",
    "            buffer.append(line)\n",
    "\n",
    "    candidate = re.sub(r\"\\s+\", \" \", \" \".join(buffer)).strip()\n",
    "    if 50 <= len(candidate) <= 5000 and re.search(r\"[a-z]\", candidate, re.I):\n",
    "        return candidate\n",
    "\n",
    "    paragraphs = re.split(r\"\\n\\s*\\n\", normalized)\n",
    "    for paragraph in paragraphs[:8]:\n",
    "        p = re.sub(r\"\\s+\", \" \", paragraph.strip())\n",
    "        if (\n",
    "            120 <= len(p) <= 5000\n",
    "            and not re.match(\n",
    "                r\"(?i)^(keywords|index\\s*terms|introduction|references|acknowledg(e)?ments)\",\n",
    "                p,\n",
    "            )\n",
    "            and p.count(\".\") >= 2\n",
    "        ):\n",
    "            return p\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "# -------- Content cleaning --------\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    if not text:\n",
    "        return \"\"\n",
    "\n",
    "    text = re.sub(r\"(\\w+)-\\s*\\n\\s*(\\w+)\", r\"\\1\\2\", text)\n",
    "    text = text.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")\n",
    "    text = re.sub(r\"(\\.{5,}|\\-{5,})\", \" \", text)\n",
    "    text = re.sub(r\"[\\t\\f\\u00A0]+\", \" \", text)\n",
    "    text = re.sub(r\"\\n{3,}\", \"\\n\\n\", text)\n",
    "\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "# -------- Chunking --------\n",
    "\n",
    "def chunk_text(\n",
    "    text: str,\n",
    "    chunk_size: int = CHUNK_SIZE,\n",
    "    overlap: int = CHUNK_OVERLAP,\n",
    ") -> List[str]:\n",
    "    if not text:\n",
    "        return []\n",
    "\n",
    "    paragraphs = text.split(\"\\n\\n\")\n",
    "    chunks = []\n",
    "    current = \"\"\n",
    "\n",
    "    for para in paragraphs:\n",
    "        para = para.strip()\n",
    "        if not para:\n",
    "            continue\n",
    "\n",
    "        if len(current) + len(para) < chunk_size:\n",
    "            current = f\"{current}\\n\\n{para}\" if current else para\n",
    "        else:\n",
    "            if current:\n",
    "                chunks.append(current)\n",
    "                current = current[-overlap:] + \"\\n\\n\" + para\n",
    "\n",
    "            if len(current) > chunk_size:\n",
    "                for i in range(0, len(current), chunk_size - overlap):\n",
    "                    chunks.append(current[i : i + chunk_size])\n",
    "                current = \"\"\n",
    "\n",
    "    if current:\n",
    "        chunks.append(current)\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "# -------- Safe file reading --------\n",
    "\n",
    "def safe_read_fulltext(file_path: str) -> str:\n",
    "    if not file_path:\n",
    "        return \"\"\n",
    "\n",
    "    decoded = urllib.parse.unquote(file_path)\n",
    "    return PDFExtractor.read_file(decoded) or \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "88b378d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List\n",
    "\n",
    "def split_sentences(text: str) -> List[str]:\n",
    "    return re.split(r'(?<=[.!?])\\s+', text)\n",
    "\n",
    "\n",
    "def extract_paragraph_chunks(\n",
    "    text: str,\n",
    "    min_len: int = 500,\n",
    "    max_len: int = 1500,\n",
    "    overlap: int = 200,\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    Paragraph-first chunking with sentence-aware fallback splitting.\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return []\n",
    "\n",
    "    paragraphs = [p.strip() for p in text.split(\"\\n\\n\") if p.strip()]\n",
    "    chunks = []\n",
    "\n",
    "    for para in paragraphs:\n",
    "        # ---- 基本过滤 ----\n",
    "        if len(para) < min_len:\n",
    "            continue\n",
    "\n",
    "        if re.search(r\"(\\.{5,}|\\-{5,})\", para):\n",
    "            continue\n",
    "\n",
    "        # ---- 正常段落：直接作为 chunk ----\n",
    "        if len(para) <= max_len:\n",
    "            chunks.append(para)\n",
    "            continue\n",
    "\n",
    "        # ---- 超长段落：sentence-aware fallback ----\n",
    "        sentences = split_sentences(para)\n",
    "\n",
    "        current = \"\"\n",
    "        for sent in sentences:\n",
    "            if len(current) + len(sent) <= max_len:\n",
    "                current = f\"{current} {sent}\".strip()\n",
    "            else:\n",
    "                if current:\n",
    "                    chunks.append(current)\n",
    "                # overlap（句子级）\n",
    "                current = sent[-overlap:] if overlap > 0 else sent\n",
    "\n",
    "        if current:\n",
    "            chunks.append(current)\n",
    "\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "584ea203",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata rows: 2\n",
      "Fulltext chunks: 65\n"
     ]
    }
   ],
   "source": [
    "metadata_rows = []\n",
    "fulltext_rows = []\n",
    "\n",
    "for paper in new_papers:\n",
    "    raw_full_text = safe_read_fulltext(paper.get(\"file_attachments\", \"\"))\n",
    "\n",
    "    full_text = clean_text(raw_full_text)\n",
    "\n",
    "    abstract = paper.get(\"abstract\", \"\")\n",
    "    if not abstract and len(full_text) >= MIN_FULLTEXT_LEN:\n",
    "        abstract = extract_abstract_from_text(full_text) or \"\"\n",
    "\n",
    "    metadata_rows.append(\n",
    "        {\n",
    "            \"paper_id\": paper[\"paper_id\"],\n",
    "            \"title\": paper[\"title\"],\n",
    "            \"abstract\": abstract,\n",
    "            \"authors\": paper[\"authors\"],\n",
    "            \"year\": paper[\"year\"],\n",
    "            \"item_type\": paper[\"item_type\"],\n",
    "            \"combined_text\": (\n",
    "                f\"Title: {paper['title']}\\n\"\n",
    "                f\"Abstract: {abstract}\"\n",
    "            ),\n",
    "        }\n",
    "    )\n",
    "\n",
    "    if len(full_text) >= MIN_FULLTEXT_LEN:\n",
    "        for i, chunk in enumerate(extract_paragraph_chunks(full_text)):\n",
    "            fulltext_rows.append(\n",
    "                {\n",
    "                    \"paper_id\": paper[\"paper_id\"],\n",
    "                    \"chunk_index\": i,\n",
    "                    \"content\": chunk,\n",
    "                    \"year\": paper[\"year\"],\n",
    "                }\n",
    "            )\n",
    "\n",
    "print(f\"Metadata rows: {len(metadata_rows)}\")\n",
    "print(f\"Fulltext chunks: {len(fulltext_rows)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eace156b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata rows: 2\n",
      "Chunk rows: 65\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1117364da544bfd843e86732ab0cb45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata embeddings generated.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0fc0f5af1bd48dea2284d133c53ff4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk embeddings generated.\n",
      "Embedding indexes created.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paper_id</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>authors</th>\n",
       "      <th>year</th>\n",
       "      <th>item_type</th>\n",
       "      <th>combined_text</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9Q8Z9AE3</td>\n",
       "      <td>H-LDM: Hierarchical Latent Diffusion Models fo...</td>\n",
       "      <td>Phonocardiogram (PCG) analysis is vital for ca...</td>\n",
       "      <td>Xu, Chenyang; Li, Siming; Wang, Hao</td>\n",
       "      <td>2025</td>\n",
       "      <td>preprint</td>\n",
       "      <td>Title: H-LDM: Hierarchical Latent Diffusion Mo...</td>\n",
       "      <td>[-0.0006771824, -0.10957697, -0.008515655, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>J8DDHQUU</td>\n",
       "      <td>Prototyping an End-to-End Multi-Modal Tiny-CNN...</td>\n",
       "      <td>The vast majority of cardiovascular diseases m...</td>\n",
       "      <td>Ibrahim, Mustafa Fuad Rifet; Alkanat, Tunc; Me...</td>\n",
       "      <td>2025</td>\n",
       "      <td>preprint</td>\n",
       "      <td>Title: Prototyping an End-to-End Multi-Modal T...</td>\n",
       "      <td>[-0.06923509, 0.018779349, 0.04884383, 0.06390...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   paper_id                                              title  \\\n",
       "0  9Q8Z9AE3  H-LDM: Hierarchical Latent Diffusion Models fo...   \n",
       "1  J8DDHQUU  Prototyping an End-to-End Multi-Modal Tiny-CNN...   \n",
       "\n",
       "                                            abstract  \\\n",
       "0  Phonocardiogram (PCG) analysis is vital for ca...   \n",
       "1  The vast majority of cardiovascular diseases m...   \n",
       "\n",
       "                                             authors  year item_type  \\\n",
       "0                Xu, Chenyang; Li, Siming; Wang, Hao  2025  preprint   \n",
       "1  Ibrahim, Mustafa Fuad Rifet; Alkanat, Tunc; Me...  2025  preprint   \n",
       "\n",
       "                                       combined_text  \\\n",
       "0  Title: H-LDM: Hierarchical Latent Diffusion Mo...   \n",
       "1  Title: Prototyping an End-to-End Multi-Modal T...   \n",
       "\n",
       "                                           embedding  \n",
       "0  [-0.0006771824, -0.10957697, -0.008515655, 0.0...  \n",
       "1  [-0.06923509, 0.018779349, 0.04884383, 0.06390...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paper_id</th>\n",
       "      <th>chunk_index</th>\n",
       "      <th>content</th>\n",
       "      <th>year</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9Q8Z9AE3</td>\n",
       "      <td>0</td>\n",
       "      <td>H-LDM: Hierarchical Latent Diffusion Models fo...</td>\n",
       "      <td>2025</td>\n",
       "      <td>[-0.017158188, -0.06888803, 0.008421682, 0.005...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9Q8Z9AE3</td>\n",
       "      <td>1</td>\n",
       "      <td>H-LDM establishes a new\\ndirection for data au...</td>\n",
       "      <td>2025</td>\n",
       "      <td>[-0.01754292, -0.096551366, -0.03906515, -0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9Q8Z9AE3</td>\n",
       "      <td>2</td>\n",
       "      <td>Conventional data augmentation\\ntechniques fai...</td>\n",
       "      <td>2025</td>\n",
       "      <td>[-0.025664022, -0.13114122, 0.044948883, 0.036...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9Q8Z9AE3</td>\n",
       "      <td>3</td>\n",
       "      <td>hical conditioning mechanism that fuses rich s...</td>\n",
       "      <td>2025</td>\n",
       "      <td>[-0.018627482, -0.081789196, 0.03577644, 0.037...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9Q8Z9AE3</td>\n",
       "      <td>4</td>\n",
       "      <td>clinical attributes. The implications of this ...</td>\n",
       "      <td>2025</td>\n",
       "      <td>[-0.061234742, -0.11351126, 0.018123308, 0.004...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   paper_id  chunk_index                                            content  \\\n",
       "0  9Q8Z9AE3            0  H-LDM: Hierarchical Latent Diffusion Models fo...   \n",
       "1  9Q8Z9AE3            1  H-LDM establishes a new\\ndirection for data au...   \n",
       "2  9Q8Z9AE3            2  Conventional data augmentation\\ntechniques fai...   \n",
       "3  9Q8Z9AE3            3  hical conditioning mechanism that fuses rich s...   \n",
       "4  9Q8Z9AE3            4  clinical attributes. The implications of this ...   \n",
       "\n",
       "   year                                          embedding  \n",
       "0  2025  [-0.017158188, -0.06888803, 0.008421682, 0.005...  \n",
       "1  2025  [-0.01754292, -0.096551366, -0.03906515, -0.00...  \n",
       "2  2025  [-0.025664022, -0.13114122, 0.044948883, 0.036...  \n",
       "3  2025  [-0.018627482, -0.081789196, 0.03577644, 0.037...  \n",
       "4  2025  [-0.061234742, -0.11351126, 0.018123308, 0.004...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_metadata = pd.DataFrame(metadata_rows)\n",
    "df_chunks = pd.DataFrame(fulltext_rows)\n",
    "\n",
    "print(f\"Metadata rows: {len(df_metadata)}\")\n",
    "print(f\"Chunk rows: {len(df_chunks)}\")\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# 3. Generate metadata embeddings\n",
    "# -------------------------\n",
    "\n",
    "if not df_metadata.empty:\n",
    "    embeddings = model.encode(\n",
    "        df_metadata[\"combined_text\"].tolist(),\n",
    "        show_progress_bar=True,\n",
    "        convert_to_numpy=True,\n",
    "    ).astype(\"float32\")\n",
    "\n",
    "    df_metadata[\"embedding\"] = list(embeddings)\n",
    "else:\n",
    "    df_metadata[\"embedding\"] = []\n",
    "\n",
    "print(\"Metadata embeddings generated.\")\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# 4. Generate full-text chunk embeddings\n",
    "# -------------------------\n",
    "\n",
    "if not df_chunks.empty:\n",
    "    embeddings = model.encode(\n",
    "        df_chunks[\"content\"].fillna(\"\").tolist(),\n",
    "        show_progress_bar=True,\n",
    "        convert_to_numpy=True,\n",
    "    ).astype(\"float32\")\n",
    "\n",
    "    df_chunks[\"embedding\"] = list(embeddings)\n",
    "\n",
    "else:\n",
    "    df_chunks[\"embedding\"] = []\n",
    "\n",
    "print(\"Chunk embeddings generated.\")\n",
    "\n",
    "# -------------------------\n",
    "# 5. Create embedding indexes\n",
    "# -------------------------\n",
    "\n",
    "metadata_index = embedding.EmbeddingIndex()\n",
    "metadata_index.add_embedding(\n",
    "    \"metadata_embedding\",\n",
    "    embedding_dim,\n",
    ")\n",
    "\n",
    "chunk_index = embedding.EmbeddingIndex()\n",
    "chunk_index.add_embedding(\n",
    "    \"chunk_embedding\",\n",
    "    embedding_dim,\n",
    ")\n",
    "\n",
    "print(\"Embedding indexes created.\")\n",
    "\n",
    "# -------------------------\n",
    "# 7. Final sanity check\n",
    "# -------------------------\n",
    "\n",
    "display(df_metadata.head())\n",
    "display(df_chunks.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "55b9b9b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading Dataframe: 100.00% |██████████| Rows 2/2 | Elapsed Time: 00:00 | Remaining Time: 00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching job: paper_metadata_fg_2_2_offline_fg_materialization\n",
      "Job started successfully, you can follow the progress at \n",
      "https://c.app.hopsworks.ai:443/p/1286333/jobs/named/paper_metadata_fg_2_2_offline_fg_materialization/executions\n",
      "2026-01-11 18:17:18,269 INFO: Waiting for execution to finish. Current state: INITIALIZING. Final status: UNDEFINED\n",
      "2026-01-11 18:17:21,464 INFO: Waiting for execution to finish. Current state: SUBMITTED. Final status: UNDEFINED\n",
      "2026-01-11 18:17:24,642 INFO: Waiting for execution to finish. Current state: RUNNING. Final status: UNDEFINED\n",
      "2026-01-11 18:19:03,471 INFO: Waiting for execution to finish. Current state: FINISHED. Final status: SUCCEEDED\n",
      "2026-01-11 18:19:03,939 INFO: Waiting for log aggregation to finish.\n",
      "2026-01-11 18:19:03,939 INFO: Execution finished successfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Job('paper_metadata_fg_2_2_offline_fg_materialization', 'SPARK'), None)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paper_fg.insert(\n",
    "    df_metadata,\n",
    "    write_options={\n",
    "        \"wait_for_job\": True,\n",
    "        \"upsert\": True,\n",
    "    },\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5b89d173",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading Dataframe: 100.00% |██████████| Rows 65/65 | Elapsed Time: 00:01 | Remaining Time: 00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching job: paper_chunk_fg_2_2_offline_fg_materialization\n",
      "Job started successfully, you can follow the progress at \n",
      "https://c.app.hopsworks.ai:443/p/1286333/jobs/named/paper_chunk_fg_2_2_offline_fg_materialization/executions\n",
      "2026-01-11 18:21:37,544 INFO: Waiting for execution to finish. Current state: INITIALIZING. Final status: UNDEFINED\n",
      "2026-01-11 18:21:40,740 INFO: Waiting for execution to finish. Current state: SUBMITTED. Final status: UNDEFINED\n",
      "2026-01-11 18:21:43,927 INFO: Waiting for execution to finish. Current state: RUNNING. Final status: UNDEFINED\n",
      "2026-01-11 18:23:32,432 INFO: Waiting for execution to finish. Current state: FINISHED. Final status: SUCCEEDED\n",
      "2026-01-11 18:23:32,892 INFO: Waiting for log aggregation to finish.\n",
      "2026-01-11 18:23:32,892 INFO: Execution finished successfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Job('paper_chunk_fg_2_2_offline_fg_materialization', 'SPARK'), None)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk_fg.insert(\n",
    "    df_chunks,\n",
    "    write_options={\n",
    "        \"wait_for_job\": True,\n",
    "        \"upsert\": True,\n",
    "    },\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aq",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
