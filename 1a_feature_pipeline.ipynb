{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d86f0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import hopsworks\n",
    "import hsfs\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from hsfs.embedding import EmbeddingIndex\n",
    "\n",
    "from functions.zotero_parser import ZoteroCSVParser\n",
    "from functions.PDF_extractor import PDFExtractor\n",
    "import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33fdd29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "ZOTERO_CSV_PATH = \"PCG_latest.csv\"\n",
    "\n",
    "# Hopsworks\n",
    "FEATURE_GROUP_PAPER = \"paper_metadata_fg_2\"\n",
    "FEATURE_GROUP_CHUNK = \"paper_chunk_fg_2\"\n",
    "\n",
    "# Embedding\n",
    "from config import EMBEDDING_MODEL_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b008d32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed 17 papers.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'paper_id': 'CLGNKPIJ',\n",
       "  'title': 'Synthesis of Normal Heart Sounds Using Generative Adversarial Networks and Empirical Wavelet Transform',\n",
       "  'authors': 'Narváez, Pedro; Percybrooks, Winston S.',\n",
       "  'year': 2020,\n",
       "  'abstract': 'Currently, there are many works in the literature focused on the analysis of heart sounds, speciﬁcally on the development of intelligent systems for the classiﬁcation of normal and abnormal heart sounds. However, the available heart sound databases are not yet large enough to train generalized machine learning models. Therefore, there is interest in the development of algorithms capable of generating heart sounds that could augment current databases. In this article, we propose a model based on generative adversary networks (GANs) to generate normal synthetic heart sounds. Additionally, a denoising algorithm is implemented using the empirical wavelet transform (EWT), allowing a decrease in the number of epochs and the computational cost that the GAN model requires. A distortion metric (mel–cepstral distortion) was used to objectively assess the quality of synthetic heart sounds. The proposed method was favorably compared with a mathematical model that is based on the morphology of the phonocardiography (PCG) signal published as the state of the art. Additionally, diﬀerent heart sound classiﬁcation models proposed as state-of-the-art were also used to test the performance of such models when the GAN-generated synthetic signals were used as test dataset. In this experiment, good accuracy results were obtained with most of the implemented models, suggesting that the GAN-generated sounds correctly capture the characteristics of natural heart sounds.',\n",
       "  'item_type': 'journalArticle',\n",
       "  'file_attachments': 'C:\\\\Users\\\\alyssa\\\\Zotero\\\\storage\\\\J6CFPY56\\\\Narváez和Percybrooks - 2020 - Synthesis of Normal Heart Sounds Using Generative Adversarial Networks and Empirical Wavelet Transfo.pdf',\n",
       "  'url': 'https://www.mdpi.com/2076-3417/10/19/7003'},\n",
       " {'paper_id': 'R7JAHFY6',\n",
       "  'title': 'ECG Generation Based on Denoising Diffusion Probabilistic Models',\n",
       "  'authors': 'Wang, Zhongyu; Ma, Caiyun; Zhao, Minghui; Zhang, Shuo; Li, Jianqing; Liu, Chengyu',\n",
       "  'year': 2024,\n",
       "  'abstract': 'Arrhythmia diseases seriously damage people’s life and health, and identifying abnormal points in ECG signals by deep neural networks is an effective method for detecting arrhythmias. However, their accuracy is often limited by the biased data distribution of the training set, and a large number of labeled ECG signals are usually harder to obtain. Therefore, this paper proposes to synthesize virtual heart beat data by denoising diffusion probability model (DDPM) based on the MIT-BIH arrhythmia database to complement the real data. Three different methods for generating heartbeat signals are also used, which are (i) generating heartbeat signals directly, (ii) generating time-frequency maps of heartbeats and transforming them into heartbeat signals, and (iii) generating sub-signals of heartbeats and fusing them into complete heartbeat signals. Regarding the evaluation of the synthesized signals, we compare the advantages and disadvantages of the three heartbeat generation methods by four metrics: DTW, PCC, ED and KLD. The experimental results showed that the optimum values of 4.37, 17.09, 0.972 and 0.0094 were obtained for ED, DTW of method (i) and PCC, KLD of method (iii), respectively.',\n",
       "  'item_type': 'conferencePaper',\n",
       "  'file_attachments': 'C:\\\\Users\\\\alyssa\\\\Zotero\\\\storage\\\\JZPFTQVT\\\\Wang 等 - 2024 - ECG Generation Based on Denoising Diffusion Probabilistic Models.pdf',\n",
       "  'url': 'https://www.cinc.org/archives/2024/pdf/CinC2024-027.pdf'}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from functions.metadata_check import sanitize_paper_metadata\n",
    "\n",
    "parser = ZoteroCSVParser(ZOTERO_CSV_PATH)\n",
    "raw_papers = parser.parse()\n",
    "\n",
    "papers_df = []\n",
    "for paper in raw_papers:\n",
    "    fixed = sanitize_paper_metadata(paper)\n",
    "    if fixed is not None:\n",
    "        papers_df.append(fixed)\n",
    "\n",
    "print(f\"Parsed {len(papers_df)} papers.\")\n",
    "papers_df[:2]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c2ffa72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-11 19:07:05,762 INFO: Use pytorch device_name: cpu\n",
      "2026-01-11 19:07:05,763 INFO: Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "Loaded embedding model: all-MiniLM-L6-v2\n",
      "Embedding dimension: 384\n"
     ]
    }
   ],
   "source": [
    "# === Cell 5: Generate Embeddings for Metadata and Full Text ===\n",
    "\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from hsfs import embedding\n",
    "\n",
    "# -------------------------\n",
    "# 1. Load embedding model\n",
    "# -------------------------\n",
    "\n",
    "model = SentenceTransformer(EMBEDDING_MODEL_NAME)\n",
    "embedding_dim = model.get_sentence_embedding_dimension()\n",
    "\n",
    "print(f\"Loaded embedding model: {EMBEDDING_MODEL_NAME}\")\n",
    "print(f\"Embedding dimension: {embedding_dim}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d127fe4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-11 19:07:10,708 INFO: Initializing external client\n",
      "2026-01-11 19:07:10,709 INFO: Base URL: https://c.app.hopsworks.ai:443\n",
      "2026-01-11 19:07:15,643 INFO: Python Engine initialized.\n",
      "\n",
      "Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1286333\n"
     ]
    }
   ],
   "source": [
    "from config import HOPSWORKS_API_KEY\n",
    "# project = hopsworks.login()\n",
    "\n",
    "project = hopsworks.login(\n",
    "        # project=HOPSWORKS_PROJECT,\n",
    "        api_key_value=HOPSWORKS_API_KEY\n",
    "    )\n",
    "fs = project.get_feature_store()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a66dedff",
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_fg = fs.get_feature_group(\n",
    "    name=FEATURE_GROUP_PAPER,\n",
    "    version=2\n",
    ")\n",
    "\n",
    "chunk_fg = fs.get_feature_group(\n",
    "    name=FEATURE_GROUP_CHUNK,\n",
    "    version=2\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "888d2e28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (1.49s) \n",
      "Papers already in Feature Store: 17\n"
     ]
    }
   ],
   "source": [
    "existing_papers_df = paper_fg.read(online=False)[[\"paper_id\"]]\n",
    "\n",
    "existing_paper_ids = set(existing_papers_df[\"paper_id\"].astype(str))\n",
    "\n",
    "print(f\"Papers already in Feature Store: {len(existing_paper_ids)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ddbfc32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New papers detected: 0\n"
     ]
    }
   ],
   "source": [
    "new_papers = []\n",
    "\n",
    "for paper in papers_df:   \n",
    "    paper_id = str(paper[\"paper_id\"])\n",
    "    if paper_id not in existing_paper_ids:\n",
    "        new_papers.append(paper)\n",
    "\n",
    "print(f\"New papers detected: {len(new_papers)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4225edb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- Safe file reading --------\n",
    "import urllib.parse\n",
    "\n",
    "def safe_read_fulltext(file_path: str) -> str:\n",
    "    if not file_path:\n",
    "        return \"\"\n",
    "\n",
    "    decoded = urllib.parse.unquote(file_path)\n",
    "    return PDFExtractor.read_file(decoded) or \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584ea203",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata rows: 2\n",
      "Fulltext chunks: 65\n"
     ]
    }
   ],
   "source": [
    "from functions.test_and_file_processor import clean_text, extract_abstract_from_text, extract_paragraph_chunks\n",
    "from config import MIN_FULLTEXT_LEN\n",
    "\n",
    "metadata_rows = []\n",
    "fulltext_rows = []\n",
    "\n",
    "for paper in new_papers:\n",
    "    raw_full_text = safe_read_fulltext(paper.get(\"file_attachments\", \"\"))\n",
    "\n",
    "    full_text = clean_text(raw_full_text)\n",
    "\n",
    "    abstract = paper.get(\"abstract\", \"\")\n",
    "    if not abstract and len(full_text) >= MIN_FULLTEXT_LEN:\n",
    "        abstract = extract_abstract_from_text(full_text) or \"\"\n",
    "\n",
    "    metadata_rows.append(\n",
    "        {\n",
    "            \"paper_id\": paper[\"paper_id\"],\n",
    "            \"title\": paper[\"title\"],\n",
    "            \"abstract\": abstract,\n",
    "            \"authors\": paper[\"authors\"],\n",
    "            \"year\": paper[\"year\"],\n",
    "            \"item_type\": paper[\"item_type\"],\n",
    "            \"combined_text\": (\n",
    "                f\"Title: {paper['title']}\\n\"\n",
    "                f\"Abstract: {abstract}\"\n",
    "            ),\n",
    "        }\n",
    "    )\n",
    "\n",
    "    if len(full_text) >= MIN_FULLTEXT_LEN:\n",
    "        for i, chunk in enumerate(extract_paragraph_chunks(full_text)):\n",
    "            fulltext_rows.append(\n",
    "                {\n",
    "                    \"paper_id\": paper[\"paper_id\"],\n",
    "                    \"chunk_index\": i,\n",
    "                    \"content\": chunk,\n",
    "                    \"year\": paper[\"year\"],\n",
    "                }\n",
    "            )\n",
    "\n",
    "print(f\"Metadata rows: {len(metadata_rows)}\")\n",
    "print(f\"Fulltext chunks: {len(fulltext_rows)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eace156b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata rows: 2\n",
      "Chunk rows: 65\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1117364da544bfd843e86732ab0cb45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata embeddings generated.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0fc0f5af1bd48dea2284d133c53ff4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk embeddings generated.\n",
      "Embedding indexes created.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paper_id</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>authors</th>\n",
       "      <th>year</th>\n",
       "      <th>item_type</th>\n",
       "      <th>combined_text</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9Q8Z9AE3</td>\n",
       "      <td>H-LDM: Hierarchical Latent Diffusion Models fo...</td>\n",
       "      <td>Phonocardiogram (PCG) analysis is vital for ca...</td>\n",
       "      <td>Xu, Chenyang; Li, Siming; Wang, Hao</td>\n",
       "      <td>2025</td>\n",
       "      <td>preprint</td>\n",
       "      <td>Title: H-LDM: Hierarchical Latent Diffusion Mo...</td>\n",
       "      <td>[-0.0006771824, -0.10957697, -0.008515655, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>J8DDHQUU</td>\n",
       "      <td>Prototyping an End-to-End Multi-Modal Tiny-CNN...</td>\n",
       "      <td>The vast majority of cardiovascular diseases m...</td>\n",
       "      <td>Ibrahim, Mustafa Fuad Rifet; Alkanat, Tunc; Me...</td>\n",
       "      <td>2025</td>\n",
       "      <td>preprint</td>\n",
       "      <td>Title: Prototyping an End-to-End Multi-Modal T...</td>\n",
       "      <td>[-0.06923509, 0.018779349, 0.04884383, 0.06390...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   paper_id                                              title  \\\n",
       "0  9Q8Z9AE3  H-LDM: Hierarchical Latent Diffusion Models fo...   \n",
       "1  J8DDHQUU  Prototyping an End-to-End Multi-Modal Tiny-CNN...   \n",
       "\n",
       "                                            abstract  \\\n",
       "0  Phonocardiogram (PCG) analysis is vital for ca...   \n",
       "1  The vast majority of cardiovascular diseases m...   \n",
       "\n",
       "                                             authors  year item_type  \\\n",
       "0                Xu, Chenyang; Li, Siming; Wang, Hao  2025  preprint   \n",
       "1  Ibrahim, Mustafa Fuad Rifet; Alkanat, Tunc; Me...  2025  preprint   \n",
       "\n",
       "                                       combined_text  \\\n",
       "0  Title: H-LDM: Hierarchical Latent Diffusion Mo...   \n",
       "1  Title: Prototyping an End-to-End Multi-Modal T...   \n",
       "\n",
       "                                           embedding  \n",
       "0  [-0.0006771824, -0.10957697, -0.008515655, 0.0...  \n",
       "1  [-0.06923509, 0.018779349, 0.04884383, 0.06390...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paper_id</th>\n",
       "      <th>chunk_index</th>\n",
       "      <th>content</th>\n",
       "      <th>year</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9Q8Z9AE3</td>\n",
       "      <td>0</td>\n",
       "      <td>H-LDM: Hierarchical Latent Diffusion Models fo...</td>\n",
       "      <td>2025</td>\n",
       "      <td>[-0.017158188, -0.06888803, 0.008421682, 0.005...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9Q8Z9AE3</td>\n",
       "      <td>1</td>\n",
       "      <td>H-LDM establishes a new\\ndirection for data au...</td>\n",
       "      <td>2025</td>\n",
       "      <td>[-0.01754292, -0.096551366, -0.03906515, -0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9Q8Z9AE3</td>\n",
       "      <td>2</td>\n",
       "      <td>Conventional data augmentation\\ntechniques fai...</td>\n",
       "      <td>2025</td>\n",
       "      <td>[-0.025664022, -0.13114122, 0.044948883, 0.036...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9Q8Z9AE3</td>\n",
       "      <td>3</td>\n",
       "      <td>hical conditioning mechanism that fuses rich s...</td>\n",
       "      <td>2025</td>\n",
       "      <td>[-0.018627482, -0.081789196, 0.03577644, 0.037...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9Q8Z9AE3</td>\n",
       "      <td>4</td>\n",
       "      <td>clinical attributes. The implications of this ...</td>\n",
       "      <td>2025</td>\n",
       "      <td>[-0.061234742, -0.11351126, 0.018123308, 0.004...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   paper_id  chunk_index                                            content  \\\n",
       "0  9Q8Z9AE3            0  H-LDM: Hierarchical Latent Diffusion Models fo...   \n",
       "1  9Q8Z9AE3            1  H-LDM establishes a new\\ndirection for data au...   \n",
       "2  9Q8Z9AE3            2  Conventional data augmentation\\ntechniques fai...   \n",
       "3  9Q8Z9AE3            3  hical conditioning mechanism that fuses rich s...   \n",
       "4  9Q8Z9AE3            4  clinical attributes. The implications of this ...   \n",
       "\n",
       "   year                                          embedding  \n",
       "0  2025  [-0.017158188, -0.06888803, 0.008421682, 0.005...  \n",
       "1  2025  [-0.01754292, -0.096551366, -0.03906515, -0.00...  \n",
       "2  2025  [-0.025664022, -0.13114122, 0.044948883, 0.036...  \n",
       "3  2025  [-0.018627482, -0.081789196, 0.03577644, 0.037...  \n",
       "4  2025  [-0.061234742, -0.11351126, 0.018123308, 0.004...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_metadata = pd.DataFrame(metadata_rows)\n",
    "df_chunks = pd.DataFrame(fulltext_rows)\n",
    "\n",
    "print(f\"Metadata rows: {len(df_metadata)}\")\n",
    "print(f\"Chunk rows: {len(df_chunks)}\")\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# 3. Generate metadata embeddings\n",
    "# -------------------------\n",
    "\n",
    "if not df_metadata.empty:\n",
    "    embeddings = model.encode(\n",
    "        df_metadata[\"combined_text\"].tolist(),\n",
    "        show_progress_bar=True,\n",
    "        convert_to_numpy=True,\n",
    "    ).astype(\"float32\")\n",
    "\n",
    "    df_metadata[\"embedding\"] = list(embeddings)\n",
    "else:\n",
    "    df_metadata[\"embedding\"] = []\n",
    "\n",
    "print(\"Metadata embeddings generated.\")\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# 4. Generate full-text chunk embeddings\n",
    "# -------------------------\n",
    "\n",
    "if not df_chunks.empty:\n",
    "    embeddings = model.encode(\n",
    "        df_chunks[\"content\"].fillna(\"\").tolist(),\n",
    "        show_progress_bar=True,\n",
    "        convert_to_numpy=True,\n",
    "    ).astype(\"float32\")\n",
    "\n",
    "    df_chunks[\"embedding\"] = list(embeddings)\n",
    "\n",
    "else:\n",
    "    df_chunks[\"embedding\"] = []\n",
    "\n",
    "print(\"Chunk embeddings generated.\")\n",
    "\n",
    "# -------------------------\n",
    "# 5. Create embedding indexes\n",
    "# -------------------------\n",
    "\n",
    "metadata_index = embedding.EmbeddingIndex()\n",
    "metadata_index.add_embedding(\n",
    "    \"metadata_embedding\",\n",
    "    embedding_dim,\n",
    ")\n",
    "\n",
    "chunk_index = embedding.EmbeddingIndex()\n",
    "chunk_index.add_embedding(\n",
    "    \"chunk_embedding\",\n",
    "    embedding_dim,\n",
    ")\n",
    "\n",
    "print(\"Embedding indexes created.\")\n",
    "\n",
    "# -------------------------\n",
    "# 7. Final sanity check\n",
    "# -------------------------\n",
    "\n",
    "display(df_metadata.head())\n",
    "display(df_chunks.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "55b9b9b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading Dataframe: 100.00% |██████████| Rows 2/2 | Elapsed Time: 00:00 | Remaining Time: 00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching job: paper_metadata_fg_2_2_offline_fg_materialization\n",
      "Job started successfully, you can follow the progress at \n",
      "https://c.app.hopsworks.ai:443/p/1286333/jobs/named/paper_metadata_fg_2_2_offline_fg_materialization/executions\n",
      "2026-01-11 18:17:18,269 INFO: Waiting for execution to finish. Current state: INITIALIZING. Final status: UNDEFINED\n",
      "2026-01-11 18:17:21,464 INFO: Waiting for execution to finish. Current state: SUBMITTED. Final status: UNDEFINED\n",
      "2026-01-11 18:17:24,642 INFO: Waiting for execution to finish. Current state: RUNNING. Final status: UNDEFINED\n",
      "2026-01-11 18:19:03,471 INFO: Waiting for execution to finish. Current state: FINISHED. Final status: SUCCEEDED\n",
      "2026-01-11 18:19:03,939 INFO: Waiting for log aggregation to finish.\n",
      "2026-01-11 18:19:03,939 INFO: Execution finished successfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Job('paper_metadata_fg_2_2_offline_fg_materialization', 'SPARK'), None)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paper_fg.insert(\n",
    "    df_metadata,\n",
    "    write_options={\n",
    "        \"wait_for_job\": True,\n",
    "        \"upsert\": True,\n",
    "    },\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5b89d173",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading Dataframe: 100.00% |██████████| Rows 65/65 | Elapsed Time: 00:01 | Remaining Time: 00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching job: paper_chunk_fg_2_2_offline_fg_materialization\n",
      "Job started successfully, you can follow the progress at \n",
      "https://c.app.hopsworks.ai:443/p/1286333/jobs/named/paper_chunk_fg_2_2_offline_fg_materialization/executions\n",
      "2026-01-11 18:21:37,544 INFO: Waiting for execution to finish. Current state: INITIALIZING. Final status: UNDEFINED\n",
      "2026-01-11 18:21:40,740 INFO: Waiting for execution to finish. Current state: SUBMITTED. Final status: UNDEFINED\n",
      "2026-01-11 18:21:43,927 INFO: Waiting for execution to finish. Current state: RUNNING. Final status: UNDEFINED\n",
      "2026-01-11 18:23:32,432 INFO: Waiting for execution to finish. Current state: FINISHED. Final status: SUCCEEDED\n",
      "2026-01-11 18:23:32,892 INFO: Waiting for log aggregation to finish.\n",
      "2026-01-11 18:23:32,892 INFO: Execution finished successfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Job('paper_chunk_fg_2_2_offline_fg_materialization', 'SPARK'), None)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk_fg.insert(\n",
    "    df_chunks,\n",
    "    write_options={\n",
    "        \"wait_for_job\": True,\n",
    "        \"upsert\": True,\n",
    "    },\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aq",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
