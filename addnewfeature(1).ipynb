{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "768148e1",
   "metadata": {},
   "source": [
    "## findnewlines 函数 输入两个csv 返回增量PCG_delat.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b1d0567d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def findnewlines(file_path_new,file_path_old):\n",
    "    df1 = pd.read_csv(file_path_new)\n",
    "    df2 = pd.read_csv(file_path_old)\n",
    "    if(df1.shape[0] >= df2.shape[0]):\n",
    "        df1,df2 = df2,df1\n",
    "    \n",
    "    key_cols = df1.columns[:4]\n",
    "    rows1 = set(df1[key_cols].astype(str).apply(tuple, axis=1))\n",
    "\n",
    "    key_cols = df2.columns[:4]   # 实际上两者列名应该一致\n",
    "    mask_new = df2[key_cols].astype(str).apply(tuple, axis=1).map(lambda x: x not in rows1)\n",
    "\n",
    "    delta = df2[mask_new].copy()\n",
    "    print(\"新增行数:\", len(delta))\n",
    "    display(delta)\n",
    "\n",
    "    delta.to_csv(\"PCG_delta.csv\", index=False)\n",
    "    print(\"Saved to PCG_delta.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ca27c075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "新增行数: 2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Key</th>\n",
       "      <th>Item Type</th>\n",
       "      <th>Publication Year</th>\n",
       "      <th>Author</th>\n",
       "      <th>Title</th>\n",
       "      <th>Publication Title</th>\n",
       "      <th>ISBN</th>\n",
       "      <th>ISSN</th>\n",
       "      <th>DOI</th>\n",
       "      <th>Url</th>\n",
       "      <th>...</th>\n",
       "      <th>Programming Language</th>\n",
       "      <th>Version</th>\n",
       "      <th>System</th>\n",
       "      <th>Code</th>\n",
       "      <th>Code Number</th>\n",
       "      <th>Section</th>\n",
       "      <th>Session</th>\n",
       "      <th>Committee</th>\n",
       "      <th>History</th>\n",
       "      <th>Legislative Body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>9Q8Z9AE3</td>\n",
       "      <td>preprint</td>\n",
       "      <td>2025</td>\n",
       "      <td>Xu, Chenyang; Li, Siming; Wang, Hao</td>\n",
       "      <td>H-LDM: Hierarchical Latent Diffusion Models fo...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.48550/arXiv.2511.14312</td>\n",
       "      <td>http://arxiv.org/abs/2511.14312</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>J8DDHQUU</td>\n",
       "      <td>preprint</td>\n",
       "      <td>2025</td>\n",
       "      <td>Ibrahim, Mustafa Fuad Rifet; Alkanat, Tunc; Me...</td>\n",
       "      <td>Prototyping an End-to-End Multi-Modal Tiny-CNN...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.48550/arXiv.2510.18668</td>\n",
       "      <td>http://arxiv.org/abs/2510.18668</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 87 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Key Item Type  Publication Year  \\\n",
       "15  9Q8Z9AE3  preprint              2025   \n",
       "16  J8DDHQUU  preprint              2025   \n",
       "\n",
       "                                               Author  \\\n",
       "15                Xu, Chenyang; Li, Siming; Wang, Hao   \n",
       "16  Ibrahim, Mustafa Fuad Rifet; Alkanat, Tunc; Me...   \n",
       "\n",
       "                                                Title Publication Title ISBN  \\\n",
       "15  H-LDM: Hierarchical Latent Diffusion Models fo...               NaN  NaN   \n",
       "16  Prototyping an End-to-End Multi-Modal Tiny-CNN...               NaN  NaN   \n",
       "\n",
       "   ISSN                        DOI                              Url  ...  \\\n",
       "15  NaN  10.48550/arXiv.2511.14312  http://arxiv.org/abs/2511.14312  ...   \n",
       "16  NaN  10.48550/arXiv.2510.18668  http://arxiv.org/abs/2510.18668  ...   \n",
       "\n",
       "   Programming Language Version System Code Code Number Section  Session  \\\n",
       "15                  NaN     NaN    NaN  NaN         NaN     NaN      NaN   \n",
       "16                  NaN     NaN    NaN  NaN         NaN     NaN      NaN   \n",
       "\n",
       "    Committee  History  Legislative Body  \n",
       "15        NaN      NaN               NaN  \n",
       "16        NaN      NaN               NaN  \n",
       "\n",
       "[2 rows x 87 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to PCG_delta.csv\n"
     ]
    }
   ],
   "source": [
    "findnewlines(\"PCG1.csv\",\"PCG2.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e90bf60",
   "metadata": {},
   "source": [
    "## embedding函数，输入增量PCG_delta.csv，函数返回embedding好的dataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ced792",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed 2 papers.\n",
      "[{'paper_id': '9Q8Z9AE3', 'title': 'H-LDM: Hierarchical Latent Diffusion Models for Controllable and Interpretable PCG Synthesis from Clinical Metadata', 'authors': 'Xu, Chenyang; Li, Siming; Wang, Hao', 'year': 2025, 'abstract': 'Phonocardiogram (PCG) analysis is vital for cardiovascular disease diagnosis, yet the scarcity of labeled pathological data hinders the capability of AI systems. To bridge this, we introduce H-LDM, a Hierarchical Latent Diffusion Model for generating clinically accurate and controllable PCG signals from structured metadata. Our approach features: (1) a multi-scale VAE that learns a physiologically-disentangled latent space, separating rhythm, heart sounds, and murmurs; (2) a hierarchical text-to-biosignal pipeline that leverages rich clinical metadata for fine-grained control over 17 distinct conditions; and (3) an interpretable diffusion process guided by a novel Medical Attention module. Experiments on the PhysioNet CirCor dataset demonstrate state-of-the-art performance, achieving a Fre´chet Audio Distance of 9.7, a 92% attribute disentanglement score, and 87.1% clinical validity confirmed by cardiologists. Augmenting diagnostic models with our synthetic data improves the accuracy of rare disease classification by 11.3%. H-LDM establishes a new direction for data augmentation in cardiac diagnostics, bridging data scarcity with interpretable clinical insights.', 'item_type': 'preprint', 'file_attachments': 'C:\\\\Users\\\\alyssa\\\\Zotero\\\\storage\\\\32H4AJP9\\\\Xu 等 - 2025 - H-LDM Hierarchical Latent Diffusion Models for Controllable and Interpretable PCG Synthesis from Cl.pdf', 'url': 'http://arxiv.org/abs/2511.14312'}, {'paper_id': 'J8DDHQUU', 'title': 'Prototyping an End-to-End Multi-Modal Tiny-CNN for Cardiovascular Sensor Patches', 'authors': 'Ibrahim, Mustafa Fuad Rifet; Alkanat, Tunc; Meijer, Maurice; Manthey, Felix; Schlaefer, Alexander; Stelldinger, Peer', 'year': 2025, 'abstract': 'The vast majority of cardiovascular diseases may be preventable if early signs and risk factors are detected. Cardiovascular monitoring with body-worn sensor devices like sensor patches allows for the detection of such signs while preserving the freedom and comfort of patients. However, the analysis of the sensor data must be robust, reliable, efficient, and highly accurate. Deep learning methods can automate data interpretation, reducing the workload of clinicians. In this work, we analyze the feasibility of applying deep learning models to the classification of synchronized electrocardiogram (ECG) and phonocardiogram (PCG) recordings on resource-constrained medical edge devices. We propose a convolutional neural network with early fusion of data to solve a binary classification problem. We train and validate our model on the synchronized ECG and PCG recordings from the Physionet Challenge 2016 dataset. Our approach reduces memory footprint and compute cost by three orders of magnitude compared to the state-of-the-art while maintaining competitive accuracy. We demonstrate the applicability of our proposed model on medical edge devices by analyzing energy consumption on a microcontroller and an experimental sensor device setup, confirming that on-device inference can be more energyefficient than continuous data streaming.', 'item_type': 'preprint', 'file_attachments': 'C:\\\\Users\\\\alyssa\\\\Zotero\\\\storage\\\\8NB3BF2H\\\\Ibrahim 等 - 2025 - Prototyping an End-to-End Multi-Modal Tiny-CNN for Cardiovascular Sensor Patches.pdf', 'url': 'http://arxiv.org/abs/2510.18668'}]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from typing import Optional, Dict, Any\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from hsfs.embedding import EmbeddingIndex\n",
    "from functions.zotero_parser import ZoteroCSVParser\n",
    "\n",
    "\n",
    "def sanitize_paper_metadata(paper: Dict[str, Any]) -> Optional[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Defensive metadata sanitation.\n",
    "    Returns None if the paper is considered invalid.\n",
    "    \"\"\"\n",
    "\n",
    "    # ---- 1. Mandatory Field Validation ----\n",
    "    title = paper.get(\"title\", \"\").strip()\n",
    "    if not title:\n",
    "        return None  # Equivalent to RDF version: discard if title is missing\n",
    "\n",
    "    paper[\"title\"] = title\n",
    "\n",
    "    # ---- 2. Year Repair (Reusing regex logic from RDF) ----\n",
    "    year = paper.get(\"year\")\n",
    "    if year is None:\n",
    "        # Attempt to recover year from other metadata fields\n",
    "        for field in (\"url\", \"abstract\"):\n",
    "            text = paper.get(field, \"\")\n",
    "            match = re.search(r\"(19|20)\\d{2}\", text)\n",
    "            if match:\n",
    "                paper[\"year\"] = int(match.group())\n",
    "                break\n",
    "\n",
    "    # ---- 3. Authors Fallback ----\n",
    "    authors = paper.get(\"authors\", \"\").strip()\n",
    "    if not authors or authors.lower() == \"nan\":\n",
    "        paper[\"authors\"] = \"Unknown\"\n",
    "\n",
    "    # ---- 4. Abstract Normalization ----\n",
    "    abstract = paper.get(\"abstract\", \"\").strip()\n",
    "    if abstract.lower() in {\"nan\", \"none\"}:\n",
    "        paper[\"abstract\"] = \"\"\n",
    "\n",
    "    # ---- 5. Attachments Handling ----\n",
    "    # Preserve original state but ensure the value is a string\n",
    "    attachments = paper.get(\"file_attachments\")\n",
    "    paper[\"file_attachments\"] = str(attachments) if attachments is not None else \"\"\n",
    "\n",
    "    return paper\n",
    "\n",
    "# === Cell 3: Parse Zotero CSV ===\n",
    "parser = ZoteroCSVParser(\"PCG_delta.csv\")\n",
    "raw_papers = parser.parse()\n",
    "\n",
    "papers = []\n",
    "for paper in raw_papers:\n",
    "    fixed = sanitize_paper_metadata(paper)\n",
    "    if fixed is not None:\n",
    "        papers.append(fixed)\n",
    "\n",
    "print(f\"Parsed {len(papers)} papers.\")\n",
    "print(papers[:2])\n",
    "\n",
    "\n",
    "# ====== 配置 ======\n",
    "PROJECT_NAME = \"airconditiondetection\"   # 你日志里看到的项目名\n",
    "FG_NAME = \"zotero_csv_meta_fg\"\n",
    "FG_VERSION = 1\n",
    "\n",
    "CSV_PATH = \"PCG_delta.csv\"   # 你刚保存的增量CSV（也可以用原始全量CSV）\n",
    "MODEL_NAME = \"all-MiniLM-L6-v2\"\n",
    "\n",
    "ID_COL = \"Key\"               # 你CSV里看起来是论文唯一Key\n",
    "TITLE_COL = \"Title\"\n",
    "ABSTRACT_COL = \"Abstract\"    # 你的Zotero CSV一般有 Abstract Note/Abstract 字段；如果不是这个名字，下面会自动兜底\n",
    "AUTHOR_COL = \"Author\"\n",
    "YEAR_COL = \"Publication Year\"\n",
    "\n",
    "def pick_col(df, candidates):\n",
    "    \"\"\"从候选列名中选一个存在的\"\"\"\n",
    "    for c in candidates:\n",
    "        if c in df.columns:\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def embedding_new_lines(file_path_PCG_delta):\n",
    "    df = pd.read_csv(file_path_PCG_delta)\n",
    "    print(\"Loaded:\", df.shape)\n",
    "\n",
    "    # 自动兼容 Zotero CSV 里 abstract 的不同列名\n",
    "    abstract_col = pick_col(df, [\"Abstract\", \"Abstract Note\", \"abstract\", \"AbstractNote\"])\n",
    "    title_col = pick_col(df, [TITLE_COL, \"title\"])\n",
    "    author_col = pick_col(df, [AUTHOR_COL, \"Authors\", \"author\"])\n",
    "    year_col = pick_col(df, [YEAR_COL, \"Year\", \"year\"])\n",
    "    id_col = pick_col(df, [ID_COL, \"paper_id\", \"id\", \"ID\"])\n",
    "\n",
    "    if id_col is None or title_col is None:\n",
    "        raise ValueError(f\"找不到主键列或标题列。当前列名: {list(df.columns)[:30]} ...\")\n",
    "\n",
    "    # 组装 combined_text（没有的字段就用空）\n",
    "    def build_text(row):\n",
    "        t = str(row.get(title_col, \"\") or \"\")\n",
    "        a = str(row.get(abstract_col, \"\") or \"\") if abstract_col else \"\"\n",
    "        au = str(row.get(author_col, \"\") or \"\") if author_col else \"\"\n",
    "        y = str(row.get(year_col, \"\") or \"\") if year_col else \"\"\n",
    "        return f\"Title: {t}\\nAuthors: {au}\\nYear: {y}\\nAbstract: {a}\"\n",
    "\n",
    "    df[\"combined_text\"] = df.apply(build_text, axis=1)\n",
    "\n",
    "    # 计算 embedding\n",
    "    model = SentenceTransformer(MODEL_NAME)\n",
    "    df[\"embedding\"] = df[\"combined_text\"].fillna(\"\").apply(lambda x: model.encode(x).tolist())\n",
    "    return df,id_col\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "94b0cda3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: (2, 87)\n",
      "[-0.001899355323985219, -0.10555146634578705, -0.00418861722573638, 0.004847000818699598, 0.017331000417470932, -0.007304350845515728, -0.0669705867767334, -0.021474093198776245, 0.09013232588768005, -0.07281465083360672, -0.024480782449245453, -0.06061367690563202, -0.039333831518888474, 0.016197163611650467, -0.002743748016655445, -0.02723805420100689, 0.012326937168836594, 0.04321884363889694, -0.07688336074352264, -0.017152979969978333, 0.03107169270515442, 0.12440228462219238, 0.02820478193461895, 0.03167025372385979, -0.007569717708975077, 0.10119252651929855, 0.01305475179105997, -0.06521143764257431, -0.006049715913832188, -0.0006925948546268046, 0.10838209837675095, 0.020417004823684692, 0.05237722396850586, 0.0843726396560669, -0.11379626393318176, 0.07160447537899017, -0.008913003839552402, -0.007200879976153374, -0.06425443291664124, -0.003002593293786049, 0.0987556055188179, 0.04239178076386452, -0.011345172300934792, 0.05875229835510254, 0.05296869948506355, -0.039592452347278595, -0.08622217178344727, -0.07005693763494492, -0.0009177316096611321, 0.049850352108478546, -0.05924654379487038, -0.03012091852724552, 0.01398665364831686, 0.08870892226696014, -0.01546141505241394, 0.0414847694337368, 0.060572050511837006, 0.01919984072446823, -0.004417967051267624, -0.0729762464761734, -0.05981074646115303, -0.03368319571018219, -0.00430256687104702, -0.051866400986909866, -0.0077246418222785, -0.025544552132487297, 0.03213866055011749, 0.04960603639483452, -0.01724221743643284, -0.03306037560105324, -0.06439349800348282, 0.030637694522738457, -0.038192201405763626, 0.07601267099380493, -0.034924738109111786, 0.08036662638187408, 0.002551370533183217, -0.019879506900906563, 0.0557987317442894, -0.049127187579870224, 0.03777274116873741, 0.0385163351893425, 0.06461320072412491, 0.011982436291873455, 0.07368513196706772, 0.018319426104426384, 0.020508792251348495, 0.041461795568466187, -0.03602482005953789, -0.03874291107058525, 0.007845220156013966, -0.03609435632824898, 0.0281851664185524, -0.009616357274353504, -0.053843431174755096, 0.01008763536810875, -0.03288884833455086, -0.03609984740614891, 0.06859876215457916, 0.03982020914554596, 0.003006424056366086, 0.026274828240275383, -0.07098127156496048, 0.03138211369514465, 0.04186851903796196, -0.10878314077854156, 0.05198780819773674, 0.01017473079264164, 0.0474223792552948, 0.0050652120262384415, 0.01912866160273552, 0.03156914934515953, 0.02497963234782219, -0.0131598562002182, 0.14997637271881104, 0.03230692073702812, -0.006584895309060812, 0.022592680528759956, 0.10312467068433762, 0.06816180050373077, -0.0320388488471508, -0.07410214841365814, -0.06765846163034439, -0.034956637769937515, 0.043707799166440964, 0.009226150810718536, -0.07933644205331802, 8.8925192550546e-34, 0.014011363498866558, -0.010640883818268776, 0.0830690935254097, 0.05800913646817207, 0.03961854800581932, -0.05139996483922005, -0.09477034211158752, -0.07541552931070328, 0.039128247648477554, 0.026310542598366737, -0.012752028182148933, 0.03378221392631531, -0.09574145823717117, 0.05024224519729614, 0.016472455114126205, -0.07998720556497574, -0.0969594419002533, 0.06972049921751022, -0.018674589693546295, -0.008228120394051075, 0.002715792739763856, 0.0033978463616222143, 0.0320880301296711, -0.05448196083307266, -0.002813885919749737, 0.02487911470234394, -0.029413850978016853, -0.04356447979807854, 0.07320354878902435, -0.004232115112245083, -0.1658175140619278, -0.01810958795249462, 0.029201000928878784, -0.01273714192211628, -0.05930642411112785, -0.005619696341454983, -0.026910632848739624, -0.036678459495306015, 0.01748182624578476, -0.02692410536110401, 0.03916286677122116, 0.0003139957843814045, -0.011230207979679108, -0.08168558776378632, -0.05808929726481438, 0.02529517002403736, 0.0005021328688599169, -0.01016918569803238, 0.0007937672780826688, -0.03644559904932976, 0.027039984241127968, -0.015272828750312328, 0.0573948509991169, -0.07260213047266006, -0.0416225865483284, 0.008379622362554073, 0.018521560356020927, -0.027286922559142113, 0.033211786299943924, 0.0006208737613633275, 0.10092335194349289, 0.030077068135142326, 0.04139427840709686, 0.018775366246700287, 0.04168467968702316, 0.023936470970511436, -0.06882227212190628, 0.04149399325251579, 0.038475535809993744, 0.044310349971055984, -0.034918103367090225, 0.08092080801725388, -0.08236145973205566, -0.08215326815843582, 0.04694664105772972, -0.0379461906850338, 0.03969602286815643, -0.0469847247004509, -0.060428835451602936, -0.025601277127861977, -0.057933349162340164, 0.04017719253897667, -0.06683827191591263, 0.026600459590554237, 0.004158322233706713, -0.07266636937856674, -0.02815328724682331, -0.03392743691802025, -0.15836402773857117, -0.0339362807571888, -0.07168329507112503, 0.04524470493197441, 0.028370898216962814, 0.05578307807445526, -0.031201530247926712, -2.066959328114163e-33, -0.047831978648900986, 0.04482092708349228, 0.04670335352420807, 0.017156951129436493, 0.039460085332393646, 0.016259988769888878, 0.0219851266592741, 0.03917185962200165, 0.07224024087190628, 0.005388423800468445, 0.05822136998176575, -0.055767133831977844, -0.006994990631937981, -0.013895637355744839, 0.027964523062109947, 0.04013065993785858, -0.018926681950688362, 0.013444467447698116, 0.022114358842372894, 0.03244117647409439, -0.0014883059775456786, 0.011271740309894085, -0.03870204836130142, 0.09889263659715652, -0.03159203380346298, 0.03529132902622223, -0.03125252574682236, -0.01800733245909214, 0.04608004167675972, -0.05922412872314453, -0.031210733577609062, 0.004629682283848524, -0.04185673967003822, -0.05562157183885574, -0.1195966824889183, 0.03666291385889053, 0.00635326886549592, 0.01664431206882, -0.0506739467382431, 0.01873229444026947, 0.024798359721899033, 0.024825165048241615, -0.09551366418600082, -0.0028851726092398167, -0.035199400037527084, -0.0165425855666399, -0.03518511727452278, 0.035936083644628525, -0.02701098471879959, 0.05554492771625519, 0.058442164212465286, 0.057628925889730453, -0.046583306044340134, 0.02727867290377617, -0.08621710538864136, -0.005620819516479969, -0.05485495552420616, -0.04247502237558365, -0.05762818455696106, -0.042122773826122284, -0.08929814398288727, -0.015404288657009602, -0.028343359008431435, -0.08861202001571655, -0.0192556269466877, 0.01748104766011238, 0.09914422035217285, 0.009740223176777363, 0.04367106780409813, -0.014955254271626472, 0.05326441675424576, 0.07766333967447281, 0.012987504713237286, 0.059100713580846786, 0.09599750488996506, -0.07172855734825134, -0.04735735431313515, -0.13827882707118988, -0.06193838268518448, -0.04994489625096321, 0.0563260018825531, -0.021642252802848816, 0.03722988814115524, -0.015507408417761326, 0.010051359422504902, 0.05457409471273422, 0.06626573950052261, -0.04045255482196808, 0.004214446526020765, 0.05870317667722702, -0.0007705820025876164, 0.060741666704416275, 0.008958841674029827, 0.07133829593658447, -0.12066195160150528, -4.3537834670814846e-08, -0.06755992025136948, -0.006952433846890926, 0.026036344468593597, -0.05772995576262474, -0.05952347442507744, -0.07700277864933014, 0.0188793633133173, 0.033952370285987854, -0.005943035241216421, 0.021153679117560387, 0.09910615533590317, -0.06568155437707901, -0.07561831176280975, -0.048648081719875336, 0.00692706136032939, 0.008711677975952625, 0.009443264454603195, 0.07954543083906174, -0.004417359828948975, -0.05282401293516159, 0.048092201352119446, 0.04878687486052513, 0.017112765461206436, -0.005146906711161137, 0.15448732674121857, -0.05151987075805664, -0.033856552094221115, 0.047587230801582336, 0.03151264414191246, 0.025935903191566467, 0.06976821273565292, 0.0309840589761734, 0.022215336561203003, 0.07127216458320618, -0.012936512008309364, 0.018933499231934547, 0.05099845677614212, -0.05601945519447327, -0.08162381500005722, 0.050523169338703156, -0.0034196043852716684, 0.0037680037785321474, -0.09750472009181976, 0.006467809434980154, -0.029818668961524963, -0.049467526376247406, 0.0644100084900856, -0.0421440526843071, 0.09910710901021957, -0.026422560214996338, 0.05098314583301544, 0.03112480789422989, 0.03249295800924301, 0.038752127438783646, 0.06523267179727554, 0.06507939100265503, -0.0050287372432649136, 0.014787130057811737, 0.016565294936299324, -0.030414966866374016, 0.03704797104001045, 0.0016877803718671203, 0.006617456208914518, 0.02074987255036831]\n"
     ]
    }
   ],
   "source": [
    "dfnewline,idcol = embedding_new_lines(CSV_PATH)\n",
    "print(dfnewline['embedding'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10532794",
   "metadata": {},
   "source": [
    "## 把dataframe 上传到hopsworks的函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d1fd25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanitize_for_hopsworks(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    让 DataFrame 满足 Hopsworks/Avro 写入要求：\n",
    "    - 把 NaN/NaT 变成 None（null）\n",
    "    - 把 object 列里非 None 的值转成 str（避免出现 float NaN / 混合类型）\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # 1) NaN / NaT -> None（非常关键）\n",
    "    df = df.replace({np.nan: None})\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def upload_new_lines(df,id_col,):\n",
    "    df = sanitize_for_hopsworks(df) #是处理空值\n",
    "    # 写入 Hopsworks\n",
    "    project = hopsworks.login(\n",
    "        api_key_value=\"FWds6IvHWvbJWDyc.hKxOZK21XXgVDZ4XEIQsZdT3oPEnKAGdOMxB55BCA4J8rASk4X10A1GAHDDyjh3j\"\n",
    "    )   \n",
    "    fs = project.get_feature_store()\n",
    "\n",
    "    fg = fs.get_or_create_feature_group(\n",
    "        name=FG_NAME,\n",
    "        version=FG_VERSION,\n",
    "        description=\"Zotero CSV metadata embeddings (no PDF)\",\n",
    "        primary_key=[id_col],\n",
    "        embedding_index=EmbeddingIndex(),\n",
    "    )\n",
    "\n",
    "    fg.insert(df)\n",
    "    print(f\"Inserted {len(df)} rows into {FG_NAME}:{FG_VERSION}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "eb443079",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-11 10:42:38,489 INFO: Initializing external client\n",
      "2026-01-11 10:42:38,489 INFO: Base URL: https://c.app.hopsworks.ai:443\n",
      "2026-01-11 10:42:40,162 INFO: Python Engine initialized.\n",
      "\n",
      "Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1282196\n",
      "2026-01-11 10:42:42,241 WARNING: FeatureGroupWarning: The ingested dataframe contains upper case letters in feature names: `['Key', 'Item Type', 'Publication Year', 'Author', 'Title', 'Publication Title', 'ISBN', 'ISSN', 'DOI', 'Url', 'Abstract Note', 'Date', 'Date Added', 'Date Modified', 'Access Date', 'Pages', 'Num Pages', 'Issue', 'Volume', 'Number Of Volumes', 'Journal Abbreviation', 'Short Title', 'Series', 'Series Number', 'Series Text', 'Series Title', 'Publisher', 'Place', 'Language', 'Rights', 'Type', 'Archive', 'Archive Location', 'Library Catalog', 'Call Number', 'Extra', 'Notes', 'File Attachments', 'Link Attachments', 'Manual Tags', 'Automatic Tags', 'Editor', 'Series Editor', 'Translator', 'Contributor', 'Attorney Agent', 'Book Author', 'Cast Member', 'Commenter', 'Composer', 'Cosponsor', 'Counsel', 'Interviewer', 'Producer', 'Recipient', 'Reviewed Author', 'Scriptwriter', 'Words By', 'Guest', 'Number', 'Edition', 'Running Time', 'Scale', 'Medium', 'Artwork Size', 'Filing Date', 'Application Number', 'Assignee', 'Issuing Authority', 'Country', 'Meeting Name', 'Conference Name', 'Court', 'References', 'Reporter', 'Legal Status', 'Priority Numbers', 'Programming Language', 'Version', 'System', 'Code', 'Code Number', 'Section', 'Session', 'Committee', 'History', 'Legislative Body']`. Feature names are sanitized to lower case in the feature store.\n",
      "\n",
      "2026-01-11 10:42:42,243 WARNING: FeatureGroupWarning: The ingested dataframe contains feature names with spaces: `['Item Type', 'Publication Year', 'Publication Title', 'Abstract Note', 'Date Added', 'Date Modified', 'Access Date', 'Num Pages', 'Number Of Volumes', 'Journal Abbreviation', 'Short Title', 'Series Number', 'Series Text', 'Series Title', 'Archive Location', 'Library Catalog', 'Call Number', 'File Attachments', 'Link Attachments', 'Manual Tags', 'Automatic Tags', 'Series Editor', 'Attorney Agent', 'Book Author', 'Cast Member', 'Reviewed Author', 'Words By', 'Running Time', 'Artwork Size', 'Filing Date', 'Application Number', 'Issuing Authority', 'Meeting Name', 'Conference Name', 'Legal Status', 'Priority Numbers', 'Programming Language', 'Code Number', 'Legislative Body']`. Feature names are sanitized to use underscore '_' in the feature store.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading Dataframe: 100.00% |██████████| Rows 2/2 | Elapsed Time: 00:01 | Remaining Time: 00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching job: zotero_csv_meta_fg_1_offline_fg_materialization\n",
      "Job started successfully, you can follow the progress at \n",
      "https://c.app.hopsworks.ai:443/p/1282196/jobs/named/zotero_csv_meta_fg_1_offline_fg_materialization/executions\n",
      "Inserted 2 rows into zotero_csv_meta_fg:1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import hopsworks\n",
    "upload_new_lines(dfnewline,idcol)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc068e1",
   "metadata": {},
   "source": [
    "## 下载hopswork返回已经有的内容df,转化成csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9c4fbe93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_fgtocsv(project_name = PROJECT_NAME, fg_name = FG_NAME , fg_version =FG_VERSION ,\n",
    "                       out_path: str = \"PCG_old.csv\"):\n",
    "    project = hopsworks.login(\n",
    "        project=project_name,\n",
    "        api_key_value=\"FWds6IvHWvbJWDyc.hKxOZK21XXgVDZ4XEIQsZdT3oPEnKAGdOMxB55BCA4J8rASk4X10A1GAHDDyjh3j\"\n",
    "    )\n",
    "    fs = project.get_feature_store()\n",
    "    fg = fs.get_feature_group(fg_name, fg_version)\n",
    "    df = fg.read()\n",
    "    if \"embedding\" in df.columns:\n",
    "        df = df.drop(columns=[\"embedding\"])\n",
    "    if \"combined_text\" in df.columns:\n",
    "        df = df.drop(columns=[\"combined_text\"])\n",
    "    df.to_csv(out_path, index=False)\n",
    "    print(f\"Downloaded {len(df)} rows from {fg_name} v{fg_version} -> {out_path}\")\n",
    "    return df\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1c85043a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "airconditiondetection\n"
     ]
    }
   ],
   "source": [
    "print(PROJECT_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ec7f9448",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-11 11:08:46,565 INFO: Closing external client and cleaning up certificates.\n",
      "Connection closed.\n",
      "2026-01-11 11:08:46,569 INFO: Initializing external client\n",
      "2026-01-11 11:08:46,570 INFO: Base URL: https://c.app.hopsworks.ai:443\n",
      "2026-01-11 11:08:47,922 INFO: Python Engine initialized.\n",
      "\n",
      "Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1282196\n",
      "Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (0.76s) \n",
      "Downloaded 4 rows from zotero_csv_meta_fg v1 -> PCG_old.csv\n"
     ]
    }
   ],
   "source": [
    "df_fg = download_fgtocsv()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d0ca531d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 87)\n"
     ]
    }
   ],
   "source": [
    "print(df_fg.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aq",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
