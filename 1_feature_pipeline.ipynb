{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82622ee3",
   "metadata": {
    "id": "82622ee3"
   },
   "source": [
    "## <span style=\"color:#ff5f27\">üìù Imports </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kkvpsHNjyrra",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 34783,
     "status": "ok",
     "timestamp": 1767717940875,
     "user": {
      "displayName": "alyssa",
      "userId": "18151921927230202476"
     },
     "user_tz": -60
    },
    "id": "kkvpsHNjyrra",
    "outputId": "49355b47-0bb8-463f-ac0f-3c87f0344ba3"
   },
   "outputs": [],
   "source": [
    "!pip install -q hopsworks==4.2.10 rdflib sentence-transformers pymupdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "gCEqcKWw3nb_",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 426
    },
    "executionInfo": {
     "elapsed": 757,
     "status": "error",
     "timestamp": 1767715315868,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -60
    },
    "id": "gCEqcKWw3nb_",
    "outputId": "eb72bbe7-16b9-4a85-b6d1-9a1e691a9193"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import hopsworks\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from hsfs.embedding import EmbeddingIndex\n",
    "\n",
    "from functions.zotero_parser import ZoteroCSVParser\n",
    "from functions.PDF_extractor import PDFExtractor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oGPa_qOH0yLQ",
   "metadata": {
    "id": "oGPa_qOH0yLQ"
   },
   "source": [
    "## <span style=\"color:#ff5f27\"> Global Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "S-TH_b7bA8GT",
   "metadata": {
    "id": "S-TH_b7bA8GT"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"HOPSWORKS_API_KEY\"] = \"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f783e27e",
   "metadata": {
    "id": "f783e27e"
   },
   "source": [
    "## <span style=\"color:#ff5f27\">üß¨ Metadata and Text Extraction </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "877d820a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import Optional, Dict, Any\n",
    "\n",
    "def sanitize_paper_metadata(paper: Dict[str, Any]) -> Optional[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Defensive metadata sanitation.\n",
    "    Returns None if the paper is considered invalid.\n",
    "    \"\"\"\n",
    "\n",
    "    # ---- 1. Mandatory Field Validation ----\n",
    "    title = paper.get(\"title\", \"\").strip()\n",
    "    if not title:\n",
    "        return None  # Equivalent to RDF version: discard if title is missing\n",
    "\n",
    "    paper[\"title\"] = title\n",
    "\n",
    "    # ---- 2. Year Repair (Reusing regex logic from RDF) ----\n",
    "    year = paper.get(\"year\")\n",
    "    if year is None:\n",
    "        # Attempt to recover year from other metadata fields\n",
    "        for field in (\"url\", \"abstract\"):\n",
    "            text = paper.get(field, \"\")\n",
    "            match = re.search(r\"(19|20)\\d{2}\", text)\n",
    "            if match:\n",
    "                paper[\"year\"] = int(match.group())\n",
    "                break\n",
    "\n",
    "    # ---- 3. Authors Fallback ----\n",
    "    authors = paper.get(\"authors\", \"\").strip()\n",
    "    if not authors or authors.lower() == \"nan\":\n",
    "        paper[\"authors\"] = \"Unknown\"\n",
    "\n",
    "    # ---- 4. Abstract Normalization ----\n",
    "    abstract = paper.get(\"abstract\", \"\").strip()\n",
    "    if abstract.lower() in {\"nan\", \"none\"}:\n",
    "        paper[\"abstract\"] = \"\"\n",
    "\n",
    "    # ---- 5. Attachments Handling ----\n",
    "    # Preserve original state but ensure the value is a string\n",
    "    attachments = paper.get(\"file_attachments\")\n",
    "    paper[\"file_attachments\"] = str(attachments) if attachments is not None else \"\"\n",
    "\n",
    "    return paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "xEaWcXPP4oN8",
   "metadata": {
    "id": "xEaWcXPP4oN8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed 15 papers.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'paper_id': 'CLGNKPIJ',\n",
       "  'title': 'Synthesis of Normal Heart Sounds Using Generative Adversarial Networks and Empirical Wavelet Transform',\n",
       "  'authors': 'Narv√°ez, Pedro; Percybrooks, Winston S.',\n",
       "  'year': 2020,\n",
       "  'abstract': 'Currently, there are many works in the literature focused on the analysis of heart sounds, speciÔ¨Åcally on the development of intelligent systems for the classiÔ¨Åcation of normal and abnormal heart sounds. However, the available heart sound databases are not yet large enough to train generalized machine learning models. Therefore, there is interest in the development of algorithms capable of generating heart sounds that could augment current databases. In this article, we propose a model based on generative adversary networks (GANs) to generate normal synthetic heart sounds. Additionally, a denoising algorithm is implemented using the empirical wavelet transform (EWT), allowing a decrease in the number of epochs and the computational cost that the GAN model requires. A distortion metric (mel‚Äìcepstral distortion) was used to objectively assess the quality of synthetic heart sounds. The proposed method was favorably compared with a mathematical model that is based on the morphology of the phonocardiography (PCG) signal published as the state of the art. Additionally, diÔ¨Äerent heart sound classiÔ¨Åcation models proposed as state-of-the-art were also used to test the performance of such models when the GAN-generated synthetic signals were used as test dataset. In this experiment, good accuracy results were obtained with most of the implemented models, suggesting that the GAN-generated sounds correctly capture the characteristics of natural heart sounds.',\n",
       "  'item_type': 'journalArticle',\n",
       "  'file_attachments': 'C:\\\\Users\\\\alyssa\\\\Zotero\\\\storage\\\\J6CFPY56\\\\Narv√°ezÂíåPercybrooks - 2020 - Synthesis of Normal Heart Sounds Using Generative Adversarial Networks and Empirical Wavelet Transfo.pdf',\n",
       "  'url': 'https://www.mdpi.com/2076-3417/10/19/7003'},\n",
       " {'paper_id': 'R7JAHFY6',\n",
       "  'title': 'ECG Generation Based on Denoising Diffusion Probabilistic Models',\n",
       "  'authors': 'Wang, Zhongyu; Ma, Caiyun; Zhao, Minghui; Zhang, Shuo; Li, Jianqing; Liu, Chengyu',\n",
       "  'year': 2024,\n",
       "  'abstract': 'Arrhythmia diseases seriously damage people‚Äôs life and health, and identifying abnormal points in ECG signals by deep neural networks is an effective method for detecting arrhythmias. However, their accuracy is often limited by the biased data distribution of the training set, and a large number of labeled ECG signals are usually harder to obtain. Therefore, this paper proposes to synthesize virtual heart beat data by denoising diffusion probability model (DDPM) based on the MIT-BIH arrhythmia database to complement the real data. Three different methods for generating heartbeat signals are also used, which are (i) generating heartbeat signals directly, (ii) generating time-frequency maps of heartbeats and transforming them into heartbeat signals, and (iii) generating sub-signals of heartbeats and fusing them into complete heartbeat signals. Regarding the evaluation of the synthesized signals, we compare the advantages and disadvantages of the three heartbeat generation methods by four metrics: DTW, PCC, ED and KLD. The experimental results showed that the optimum values of 4.37, 17.09, 0.972 and 0.0094 were obtained for ED, DTW of method (i) and PCC, KLD of method (iii), respectively.',\n",
       "  'item_type': 'conferencePaper',\n",
       "  'file_attachments': 'C:\\\\Users\\\\alyssa\\\\Zotero\\\\storage\\\\JZPFTQVT\\\\Wang Á≠â - 2024 - ECG Generation Based on Denoising Diffusion Probabilistic Models.pdf',\n",
       "  'url': 'https://www.cinc.org/archives/2024/pdf/CinC2024-027.pdf'}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === Cell 3: Parse Zotero CSV ===\n",
    "parser = ZoteroCSVParser(\"PCG.csv\")\n",
    "raw_papers = parser.parse()\n",
    "\n",
    "papers = []\n",
    "for paper in raw_papers:\n",
    "    fixed = sanitize_paper_metadata(paper)\n",
    "    if fixed is not None:\n",
    "        papers.append(fixed)\n",
    "\n",
    "print(f\"Parsed {len(papers)} papers.\")\n",
    "papers[:2]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "318Tc5AI4t_T",
   "metadata": {
    "id": "318Tc5AI4t_T"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata rows: 15\n",
      "Fulltext chunks: 59\n"
     ]
    }
   ],
   "source": [
    "# === Cell 4: Extract Full Text from Attachments ===\n",
    "\n",
    "import re\n",
    "import urllib.parse\n",
    "from typing import List, Optional\n",
    "\n",
    "from config import MIN_FULLTEXT_LEN, CHUNK_SIZE, CHUNK_OVERLAP\n",
    "\n",
    "\n",
    "metadata_rows = []\n",
    "fulltext_rows = []\n",
    "\n",
    "\n",
    "# -------- Abstract extraction (provided implementation) --------\n",
    "\n",
    "def extract_abstract_from_text(text: str) -> Optional[str]:\n",
    "    normalized = text.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")\n",
    "\n",
    "    stop_markers = (\n",
    "        r\"keywords|index\\s*terms|subject[s]?|introduction|background|materials\\s+and\\s+methods|\"\n",
    "        r\"methods|results|conclusions|references|acknowledg(e)?ments|1\\.|i\\.|ii\\.|iii\\.\"\n",
    "        r\"|Keywords|Introduction|Background|Methods|Results|Conclusion|References\"\n",
    "    )\n",
    "    start_markers = r\"abstract|summary|Abstract|Summary\"\n",
    "\n",
    "    pattern = rf\"(?is)\\b(?:{start_markers})\\b\\s*[:\\.\\-]?\\s*(.+?)(?=\\n\\s*(?:{stop_markers})\\b|\\n\\n\\s*[A-Z][A-Za-z ]+\\b|\\Z)\"\n",
    "    match = re.search(pattern, normalized)\n",
    "    if match:\n",
    "        abstract = re.sub(r\"\\s+\", \" \", match.group(1).strip())\n",
    "        if 50 <= len(abstract) <= 5000 and re.search(r\"[a-z]\", abstract, re.I):\n",
    "            return abstract\n",
    "\n",
    "    lines = normalized.split(\"\\n\")\n",
    "    abstract_started = False\n",
    "    buffer: list[str] = []\n",
    "\n",
    "    for line in lines:\n",
    "        line_stripped = line.strip()\n",
    "\n",
    "        if not abstract_started:\n",
    "            if re.match(\n",
    "                r\"(?i)^(abstract|summary)\\b\\s*[:\\-\\.]?\\s*$\",\n",
    "                line_stripped,\n",
    "            ) or re.match(\n",
    "                r\"(?i)^(abstract|summary)\\b\\s*[:\\-\\.]?\",\n",
    "                line_stripped,\n",
    "            ):\n",
    "                after = re.sub(\n",
    "                    r\"(?i)^(abstract|summary)\\b\\s*[:\\-\\.]?\\s*\",\n",
    "                    \"\",\n",
    "                    line_stripped,\n",
    "                )\n",
    "                if after:\n",
    "                    buffer.append(after)\n",
    "                abstract_started = True\n",
    "            continue\n",
    "        else:\n",
    "            if re.match(rf\"(?i)^\\s*(?:{stop_markers})\\b\", line_stripped):\n",
    "                break\n",
    "            buffer.append(line)\n",
    "\n",
    "    candidate = re.sub(r\"\\s+\", \" \", \" \".join(buffer)).strip()\n",
    "    if 50 <= len(candidate) <= 5000 and re.search(r\"[a-z]\", candidate, re.I):\n",
    "        return candidate\n",
    "\n",
    "    paragraphs = re.split(r\"\\n\\s*\\n\", normalized)\n",
    "    for paragraph in paragraphs[:8]:\n",
    "        p = re.sub(r\"\\s+\", \" \", paragraph.strip())\n",
    "        if (\n",
    "            120 <= len(p) <= 5000\n",
    "            and not re.match(\n",
    "                r\"(?i)^(keywords|index\\s*terms|introduction|references|acknowledg(e)?ments)\",\n",
    "                p,\n",
    "            )\n",
    "            and p.count(\".\") >= 2\n",
    "        ):\n",
    "            return p\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "# -------- Content cleaning --------\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    if not text:\n",
    "        return \"\"\n",
    "\n",
    "    text = re.sub(r\"(\\w+)-\\s*\\n\\s*(\\w+)\", r\"\\1\\2\", text)\n",
    "    text = text.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")\n",
    "    text = re.sub(r\"(\\.{5,}|\\-{5,})\", \" \", text)\n",
    "    text = re.sub(r\"[\\t\\f\\u00A0]+\", \" \", text)\n",
    "    text = re.sub(r\"\\n{3,}\", \"\\n\\n\", text)\n",
    "\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "# -------- Chunking --------\n",
    "\n",
    "def chunk_text(\n",
    "    text: str,\n",
    "    chunk_size: int = CHUNK_SIZE,\n",
    "    overlap: int = CHUNK_OVERLAP,\n",
    ") -> List[str]:\n",
    "    if not text:\n",
    "        return []\n",
    "\n",
    "    paragraphs = text.split(\"\\n\\n\")\n",
    "    chunks = []\n",
    "    current = \"\"\n",
    "\n",
    "    for para in paragraphs:\n",
    "        para = para.strip()\n",
    "        if not para:\n",
    "            continue\n",
    "\n",
    "        if len(current) + len(para) < chunk_size:\n",
    "            current = f\"{current}\\n\\n{para}\" if current else para\n",
    "        else:\n",
    "            if current:\n",
    "                chunks.append(current)\n",
    "                current = current[-overlap:] + \"\\n\\n\" + para\n",
    "\n",
    "            if len(current) > chunk_size:\n",
    "                for i in range(0, len(current), chunk_size - overlap):\n",
    "                    chunks.append(current[i : i + chunk_size])\n",
    "                current = \"\"\n",
    "\n",
    "    if current:\n",
    "        chunks.append(current)\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "# -------- Safe file reading --------\n",
    "\n",
    "def safe_read_fulltext(file_path: str) -> str:\n",
    "    if not file_path:\n",
    "        return \"\"\n",
    "\n",
    "    decoded = urllib.parse.unquote(file_path)\n",
    "    return PDFExtractor.read_file(decoded) or \"\"\n",
    "\n",
    "\n",
    "# -------- Main loop --------\n",
    "\n",
    "for paper in papers:\n",
    "    # ---- Full text extraction ----\n",
    "    raw_full_text = safe_read_fulltext(paper.get(\"file_attachments\", \"\"))\n",
    "\n",
    "    # ---- Content processing ----\n",
    "    full_text = clean_text(raw_full_text)\n",
    "\n",
    "    # ---- Abstract handling ----\n",
    "    abstract = paper.get(\"abstract\", \"\")\n",
    "    if not abstract and len(full_text) >= MIN_FULLTEXT_LEN:\n",
    "        abstract = extract_abstract_from_text(full_text) or \"\"\n",
    "\n",
    "    # ---- Paper-level features ----\n",
    "    metadata_rows.append(\n",
    "        {\n",
    "            \"paper_id\": paper[\"paper_id\"],\n",
    "            \"title\": paper[\"title\"],\n",
    "            \"abstract\": abstract,\n",
    "            \"authors\": paper[\"authors\"],\n",
    "            \"year\": paper[\"year\"],\n",
    "            \"item_type\": paper[\"item_type\"],\n",
    "            \"combined_text\": (\n",
    "                f\"Title: {paper['title']}\\n\"\n",
    "                f\"Abstract: {abstract}\"\n",
    "            ),\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # ---- Chunk-level features ----\n",
    "    if len(full_text) >= MIN_FULLTEXT_LEN:\n",
    "        for i, chunk in enumerate(chunk_text(full_text)):\n",
    "            fulltext_rows.append(\n",
    "                {\n",
    "                    \"paper_id\": paper[\"paper_id\"],\n",
    "                    \"chunk_index\": i,\n",
    "                    \"content\": chunk,\n",
    "                    \"year\": paper[\"year\"],\n",
    "                }\n",
    "            )\n",
    "\n",
    "print(f\"Metadata rows: {len(metadata_rows)}\")\n",
    "print(f\"Fulltext chunks: {len(fulltext_rows)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3b3d28",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27;\"> üîÆ Embedding Extraction </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "862c2315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-08 01:51:49,183 INFO: Use pytorch device_name: cpu\n",
      "2026-01-08 01:51:49,186 INFO: Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "Loaded embedding model: all-MiniLM-L6-v2\n",
      "Embedding dimension: 384\n",
      "Metadata rows: 15\n",
      "Chunk rows: 59\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "593187916a754a01b0eb147efcac3395",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata embeddings generated.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3fc3193c7ad4586aa9d854cfcd638b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk embeddings generated.\n",
      "Embedding indexes created.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paper_id</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>authors</th>\n",
       "      <th>year</th>\n",
       "      <th>item_type</th>\n",
       "      <th>combined_text</th>\n",
       "      <th>embedding</th>\n",
       "      <th>context_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CLGNKPIJ</td>\n",
       "      <td>Synthesis of Normal Heart Sounds Using Generat...</td>\n",
       "      <td>Currently, there are many works in the literat...</td>\n",
       "      <td>Narv√°ez, Pedro; Percybrooks, Winston S.</td>\n",
       "      <td>2020</td>\n",
       "      <td>journalArticle</td>\n",
       "      <td>Title: Synthesis of Normal Heart Sounds Using ...</td>\n",
       "      <td>[-0.09191086, -0.067204505, 0.036164455, -0.05...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>R7JAHFY6</td>\n",
       "      <td>ECG Generation Based on Denoising Diffusion Pr...</td>\n",
       "      <td>Arrhythmia diseases seriously damage people‚Äôs ...</td>\n",
       "      <td>Wang, Zhongyu; Ma, Caiyun; Zhao, Minghui; Zhan...</td>\n",
       "      <td>2024</td>\n",
       "      <td>conferencePaper</td>\n",
       "      <td>Title: ECG Generation Based on Denoising Diffu...</td>\n",
       "      <td>[-0.032254055, -0.06744331, -0.021227641, -0.0...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>WBP9IWFM</td>\n",
       "      <td>PhysioBank, PhysioToolkit, and PhysioNet: Comp...</td>\n",
       "      <td>Abstract               ‚ÄîThe newly inaugurated ...</td>\n",
       "      <td>Goldberger, Ary L.; Amaral, Luis A. N.; Glass,...</td>\n",
       "      <td>2000</td>\n",
       "      <td>journalArticle</td>\n",
       "      <td>Title: PhysioBank, PhysioToolkit, and PhysioNe...</td>\n",
       "      <td>[-0.04702052, -0.10492723, -0.058769476, -0.00...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>UPL26UPV</td>\n",
       "      <td>Comparative Analysis of CNN and Transformer Ar...</td>\n",
       "      <td>The automated classification of phonocardiogra...</td>\n",
       "      <td>Sondermann, Martin; Bisgin, Pinar; Tschorn, Ni...</td>\n",
       "      <td>2025</td>\n",
       "      <td>preprint</td>\n",
       "      <td>Title: Comparative Analysis of CNN and Transfo...</td>\n",
       "      <td>[-0.047316596, -0.04460356, 0.037667654, -0.01...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BX8G68KQ</td>\n",
       "      <td>A Comprehensive Overview of Heart Sound Analys...</td>\n",
       "      <td>Cardiovascular diseases (CVDs) are a prevalent...</td>\n",
       "      <td>Hamza, Motaz Faroq A. Ben; Sjarif, Nilam Nur Amir</td>\n",
       "      <td>2024</td>\n",
       "      <td>journalArticle</td>\n",
       "      <td>Title: A Comprehensive Overview of Heart Sound...</td>\n",
       "      <td>[-0.021934992, -0.061657984, 0.042390797, -0.0...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   paper_id                                              title  \\\n",
       "0  CLGNKPIJ  Synthesis of Normal Heart Sounds Using Generat...   \n",
       "1  R7JAHFY6  ECG Generation Based on Denoising Diffusion Pr...   \n",
       "2  WBP9IWFM  PhysioBank, PhysioToolkit, and PhysioNet: Comp...   \n",
       "3  UPL26UPV  Comparative Analysis of CNN and Transformer Ar...   \n",
       "4  BX8G68KQ  A Comprehensive Overview of Heart Sound Analys...   \n",
       "\n",
       "                                            abstract  \\\n",
       "0  Currently, there are many works in the literat...   \n",
       "1  Arrhythmia diseases seriously damage people‚Äôs ...   \n",
       "2  Abstract               ‚ÄîThe newly inaugurated ...   \n",
       "3  The automated classification of phonocardiogra...   \n",
       "4  Cardiovascular diseases (CVDs) are a prevalent...   \n",
       "\n",
       "                                             authors  year        item_type  \\\n",
       "0            Narv√°ez, Pedro; Percybrooks, Winston S.  2020   journalArticle   \n",
       "1  Wang, Zhongyu; Ma, Caiyun; Zhao, Minghui; Zhan...  2024  conferencePaper   \n",
       "2  Goldberger, Ary L.; Amaral, Luis A. N.; Glass,...  2000   journalArticle   \n",
       "3  Sondermann, Martin; Bisgin, Pinar; Tschorn, Ni...  2025         preprint   \n",
       "4  Hamza, Motaz Faroq A. Ben; Sjarif, Nilam Nur Amir  2024   journalArticle   \n",
       "\n",
       "                                       combined_text  \\\n",
       "0  Title: Synthesis of Normal Heart Sounds Using ...   \n",
       "1  Title: ECG Generation Based on Denoising Diffu...   \n",
       "2  Title: PhysioBank, PhysioToolkit, and PhysioNe...   \n",
       "3  Title: Comparative Analysis of CNN and Transfo...   \n",
       "4  Title: A Comprehensive Overview of Heart Sound...   \n",
       "\n",
       "                                           embedding  context_id  \n",
       "0  [-0.09191086, -0.067204505, 0.036164455, -0.05...           0  \n",
       "1  [-0.032254055, -0.06744331, -0.021227641, -0.0...           1  \n",
       "2  [-0.04702052, -0.10492723, -0.058769476, -0.00...           2  \n",
       "3  [-0.047316596, -0.04460356, 0.037667654, -0.01...           3  \n",
       "4  [-0.021934992, -0.061657984, 0.042390797, -0.0...           4  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paper_id</th>\n",
       "      <th>chunk_index</th>\n",
       "      <th>content</th>\n",
       "      <th>year</th>\n",
       "      <th>embedding</th>\n",
       "      <th>context_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CLGNKPIJ</td>\n",
       "      <td>0</td>\n",
       "      <td>Appl. Sci. 2020, 10, 7003\\n7 of 16\\n \\n \\n \\nF...</td>\n",
       "      <td>2020</td>\n",
       "      <td>[-0.037153482, -0.05627889, -0.021900643, -0.0...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CLGNKPIJ</td>\n",
       "      <td>1</td>\n",
       "      <td>range of 0‚Äì200 Hz; (F) EWT component of the no...</td>\n",
       "      <td>2020</td>\n",
       "      <td>[-0.11885451, 0.0015717824, 0.016471593, 0.005...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CLGNKPIJ</td>\n",
       "      <td>2</td>\n",
       "      <td>with a relatively high noise level, as shown ...</td>\n",
       "      <td>2020</td>\n",
       "      <td>[-0.14223221, 0.012694337, 0.021791773, -0.028...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BX8G68KQ</td>\n",
       "      <td>0</td>\n",
       "      <td>M. F. A. B. Hamza, N. N. Amir Sjarif: Comprehe...</td>\n",
       "      <td>2024</td>\n",
       "      <td>[0.07923419, -0.030007407, -0.021555917, 0.031...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BX8G68KQ</td>\n",
       "      <td>1</td>\n",
       "      <td>w of Heart Sound Analysis\\nTABLE 4. Summary of...</td>\n",
       "      <td>2024</td>\n",
       "      <td>[-0.07195769, -0.09754954, 0.014396299, -0.041...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   paper_id  chunk_index                                            content  \\\n",
       "0  CLGNKPIJ            0  Appl. Sci. 2020, 10, 7003\\n7 of 16\\n \\n \\n \\nF...   \n",
       "1  CLGNKPIJ            1  range of 0‚Äì200 Hz; (F) EWT component of the no...   \n",
       "2  CLGNKPIJ            2   with a relatively high noise level, as shown ...   \n",
       "3  BX8G68KQ            0  M. F. A. B. Hamza, N. N. Amir Sjarif: Comprehe...   \n",
       "4  BX8G68KQ            1  w of Heart Sound Analysis\\nTABLE 4. Summary of...   \n",
       "\n",
       "   year                                          embedding  context_id  \n",
       "0  2020  [-0.037153482, -0.05627889, -0.021900643, -0.0...           0  \n",
       "1  2020  [-0.11885451, 0.0015717824, 0.016471593, 0.005...           1  \n",
       "2  2020  [-0.14223221, 0.012694337, 0.021791773, -0.028...           2  \n",
       "3  2024  [0.07923419, -0.030007407, -0.021555917, 0.031...           3  \n",
       "4  2024  [-0.07195769, -0.09754954, 0.014396299, -0.041...           4  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# === Cell 5: Generate Embeddings for Metadata and Full Text ===\n",
    "\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from hsfs import embedding\n",
    "\n",
    "from config import EMBEDDING_MODEL_NAME\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# 1. Load embedding model\n",
    "# -------------------------\n",
    "\n",
    "model = SentenceTransformer(EMBEDDING_MODEL_NAME)\n",
    "embedding_dim = model.get_sentence_embedding_dimension()\n",
    "\n",
    "print(f\"Loaded embedding model: {EMBEDDING_MODEL_NAME}\")\n",
    "print(f\"Embedding dimension: {embedding_dim}\")\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# 2. Prepare DataFrames\n",
    "# -------------------------\n",
    "\n",
    "df_metadata = pd.DataFrame(metadata_rows)\n",
    "df_chunks = pd.DataFrame(fulltext_rows)\n",
    "\n",
    "print(f\"Metadata rows: {len(df_metadata)}\")\n",
    "print(f\"Chunk rows: {len(df_chunks)}\")\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# 3. Generate metadata embeddings\n",
    "# -------------------------\n",
    "\n",
    "if not df_metadata.empty:\n",
    "    embeddings = model.encode(\n",
    "        df_metadata[\"combined_text\"].tolist(),\n",
    "        show_progress_bar=True,\n",
    "        convert_to_numpy=True,\n",
    "    ).astype(\"float32\")\n",
    "\n",
    "    df_metadata[\"embedding\"] = list(embeddings)\n",
    "else:\n",
    "    df_metadata[\"embedding\"] = []\n",
    "\n",
    "print(\"Metadata embeddings generated.\")\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# 4. Generate full-text chunk embeddings\n",
    "# -------------------------\n",
    "\n",
    "if not df_chunks.empty:\n",
    "    embeddings = model.encode(\n",
    "        df_chunks[\"content\"].fillna(\"\").tolist(),\n",
    "        show_progress_bar=True,\n",
    "        convert_to_numpy=True,\n",
    "    ).astype(\"float32\")\n",
    "\n",
    "    df_chunks[\"embedding\"] = list(embeddings)\n",
    "\n",
    "else:\n",
    "    df_chunks[\"embedding\"] = []\n",
    "\n",
    "print(\"Chunk embeddings generated.\")\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# 5. Add context_id (stable row id)\n",
    "# -------------------------\n",
    "\n",
    "df_metadata[\"context_id\"] = range(len(df_metadata))\n",
    "df_chunks[\"context_id\"] = range(len(df_chunks))\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# 6. Create embedding indexes\n",
    "# -------------------------\n",
    "\n",
    "metadata_index = embedding.EmbeddingIndex()\n",
    "metadata_index.add_embedding(\n",
    "    \"metadata_embedding\",\n",
    "    embedding_dim,\n",
    ")\n",
    "\n",
    "chunk_index = embedding.EmbeddingIndex()\n",
    "chunk_index.add_embedding(\n",
    "    \"chunk_embedding\",\n",
    "    embedding_dim,\n",
    ")\n",
    "\n",
    "print(\"Embedding indexes created.\")\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# 7. Final sanity check\n",
    "# -------------------------\n",
    "\n",
    "display(df_metadata.head())\n",
    "display(df_chunks.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2bced31",
   "metadata": {
    "id": "d2bced31"
   },
   "source": [
    "## <span style=\"color:#ff5f27;\"> üîÆ Connecting to Hopsworks Feature Store </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7caf764d",
   "metadata": {
    "id": "7caf764d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-08 01:51:57,836 INFO: Closing external client and cleaning up certificates.\n",
      "Connection closed.\n",
      "2026-01-08 01:51:57,839 INFO: Initializing external client\n",
      "2026-01-08 01:51:57,840 INFO: Base URL: https://c.app.hopsworks.ai:443\n",
      "2026-01-08 01:51:59,262 INFO: Python Engine initialized.\n",
      "\n",
      "Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1286333\n"
     ]
    }
   ],
   "source": [
    "import hopsworks\n",
    "from config import HOPSWORKS_API_KEY\n",
    "# project = hopsworks.login()\n",
    "\n",
    "project = hopsworks.login(\n",
    "        # project=HOPSWORKS_PROJECT,\n",
    "        api_key_value=HOPSWORKS_API_KEY\n",
    "    )\n",
    "\n",
    "fs = project.get_feature_store()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed9ac69",
   "metadata": {
    "id": "0ed9ac69"
   },
   "source": [
    "## <span style=\"color:#ff5f27;\"> ü™Ñ Feature Group Creation </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9f5e486b",
   "metadata": {
    "id": "9f5e486b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Group created successfully, explore it at \n",
      "https://c.app.hopsworks.ai:443/p/1286333/fs/1273958/fg/1908147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading Dataframe: 100.00% |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| Rows 15/15 | Elapsed Time: 00:01 | Remaining Time: 00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching job: paper_metadata_fg_1_offline_fg_materialization\n",
      "Job started successfully, you can follow the progress at \n",
      "https://c.app.hopsworks.ai:443/p/1286333/jobs/named/paper_metadata_fg_1_offline_fg_materialization/executions\n",
      "2026-01-08 01:52:28,475 INFO: Waiting for execution to finish. Current state: SUBMITTED. Final status: UNDEFINED\n",
      "2026-01-08 01:52:34,857 INFO: Waiting for execution to finish. Current state: RUNNING. Final status: UNDEFINED\n",
      "2026-01-08 01:54:20,039 INFO: Waiting for execution to finish. Current state: AGGREGATING_LOGS. Final status: SUCCEEDED\n",
      "2026-01-08 01:54:20,211 INFO: Waiting for log aggregation to finish.\n",
      "2026-01-08 01:54:28,835 INFO: Execution finished successfully.\n",
      "Inserted 15 rows into metadata feature group.\n"
     ]
    }
   ],
   "source": [
    "# === Cell 6.1: Create or Get Metadata Feature Group (Safe Version) ===\n",
    "\n",
    "from hsfs import embedding\n",
    "\n",
    "metadata_emb_index = embedding.EmbeddingIndex()\n",
    "metadata_emb_index.add_embedding(\n",
    "    \"embedding\",\n",
    "    model.get_sentence_embedding_dimension(),\n",
    ")\n",
    "\n",
    "metadata_fg = fs.get_or_create_feature_group(\n",
    "    name=\"paper_metadata_fg\",\n",
    "    version=1,\n",
    "    description=\"Paper-level metadata features (title, abstract, authors, year, item type)\",\n",
    "    primary_key=[\"paper_id\"],\n",
    "    online_enabled=True,\n",
    "    embedding_index=metadata_emb_index,\n",
    ")\n",
    "\n",
    "\n",
    "metadata_fg.insert(\n",
    "    df_metadata,\n",
    "    write_options={\"wait_for_job\": True},\n",
    ")\n",
    "\n",
    "print(f\"Inserted {len(df_metadata)} rows into metadata feature group.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6e32b548",
   "metadata": {
    "id": "6e32b548"
   },
   "outputs": [],
   "source": [
    "# === Cell 6.2: Create or Get Fulltext Chunk Feature Group (Safe Version) ===\n",
    "\n",
    "chunk_emb_index = embedding.EmbeddingIndex()\n",
    "chunk_emb_index.add_embedding(\n",
    "    \"embedding\",\n",
    "    model.get_sentence_embedding_dimension(),\n",
    ")\n",
    "\n",
    "chunk_fg = fs.get_or_create_feature_group(\n",
    "    name=\"paper_chunk_fg\",\n",
    "    version=1,\n",
    "    description=\"Chunk-level full text features for RAG\",\n",
    "    primary_key=[\"paper_id\", \"chunk_index\"],\n",
    "    online_enabled=True,\n",
    "    embedding_index=chunk_emb_index,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ca2133dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Group created successfully, explore it at \n",
      "https://c.app.hopsworks.ai:443/p/1286333/fs/1273958/fg/1893857\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading Dataframe: 100.00% |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| Rows 59/59 | Elapsed Time: 00:01 | Remaining Time: 00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching job: paper_chunk_fg_1_offline_fg_materialization\n",
      "Job started successfully, you can follow the progress at \n",
      "https://c.app.hopsworks.ai:443/p/1286333/jobs/named/paper_chunk_fg_1_offline_fg_materialization/executions\n",
      "2026-01-08 01:55:06,106 INFO: Waiting for execution to finish. Current state: INITIALIZING. Final status: UNDEFINED\n",
      "2026-01-08 01:55:09,298 INFO: Waiting for execution to finish. Current state: SUBMITTED. Final status: UNDEFINED\n",
      "2026-01-08 01:55:12,491 INFO: Waiting for execution to finish. Current state: RUNNING. Final status: UNDEFINED\n",
      "2026-01-08 01:56:57,908 INFO: Waiting for log aggregation to finish.\n",
      "2026-01-08 01:57:06,629 INFO: Execution finished successfully.\n",
      "Inserted 59 rows into chunk feature group.\n"
     ]
    }
   ],
   "source": [
    "chunk_fg.insert(\n",
    "    df_chunks,\n",
    "    write_options={\"wait_for_job\": True},\n",
    ")\n",
    "\n",
    "print(f\"Inserted {len(df_chunks)} rows into chunk feature group.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39a9ed6",
   "metadata": {
    "id": "d39a9ed6"
   },
   "source": [
    "## <span style=\"color:#ff5f27;\">ü™Ñ Feature View Creation </span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "OJ6koRqW6KsN",
   "metadata": {
    "id": "OJ6koRqW6KsN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature view created successfully, explore it at \n",
      "https://c.app.hopsworks.ai:443/p/1286333/fs/1273958/fv/paper_metadata_fv/version/1\n",
      "Metadata Feature View ready.\n"
     ]
    }
   ],
   "source": [
    "# === Cell 8.1: Create Metadata Feature View ===\n",
    "\n",
    "metadata_fv = fs.get_or_create_feature_view(\n",
    "    name=\"paper_metadata_fv\",\n",
    "    version=1,\n",
    "    description=\"Paper-level metadata for retrieval and filtering\",\n",
    "    query=metadata_fg.select(\n",
    "        [\n",
    "            \"paper_id\",\n",
    "            \"title\",\n",
    "            \"abstract\",\n",
    "            \"authors\",\n",
    "            \"year\",\n",
    "            \"item_type\",\n",
    "            \"embedding\",\n",
    "        ]\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(\"Metadata Feature View ready.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "Bj3WyVOX6MQS",
   "metadata": {
    "id": "Bj3WyVOX6MQS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature view created successfully, explore it at \n",
      "https://c.app.hopsworks.ai:443/p/1286333/fs/1273958/fv/paper_chunk_fv/version/1\n",
      "Chunk Feature View ready.\n"
     ]
    }
   ],
   "source": [
    "# === Cell 8.2: Create Chunk Feature View ===\n",
    "\n",
    "chunk_fv = fs.get_or_create_feature_view(\n",
    "    name=\"paper_chunk_fv\",\n",
    "    version=1,\n",
    "    description=\"Chunk-level full text for RAG context retrieval\",\n",
    "    query=chunk_fg.select(\n",
    "        [\n",
    "            \"paper_id\",\n",
    "            \"chunk_index\",\n",
    "            \"content\",\n",
    "            \"year\",\n",
    "            \"embedding\",\n",
    "        ]\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(\"Chunk Feature View ready.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708b9a5f",
   "metadata": {
    "id": "708b9a5f"
   },
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "https://github.com/alyssa2024/ID2223-Project/blob/main/1_feature_backfill.ipynb",
     "timestamp": 1767715450010
    }
   ]
  },
  "kernelspec": {
   "display_name": "aq",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
