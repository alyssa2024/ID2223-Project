{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82622ee3",
   "metadata": {
    "id": "82622ee3"
   },
   "source": [
    "## <span style=\"color:#ff5f27\">üìù Imports </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "kkvpsHNjyrra",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 34783,
     "status": "ok",
     "timestamp": 1767717940875,
     "user": {
      "displayName": "alyssa",
      "userId": "18151921927230202476"
     },
     "user_tz": -60
    },
    "id": "kkvpsHNjyrra",
    "outputId": "49355b47-0bb8-463f-ac0f-3c87f0344ba3"
   },
   "outputs": [],
   "source": [
    "!pip install -q hopsworks==4.2.10 rdflib sentence-transformers pymupdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "gCEqcKWw3nb_",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 426
    },
    "executionInfo": {
     "elapsed": 757,
     "status": "error",
     "timestamp": 1767715315868,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -60
    },
    "id": "gCEqcKWw3nb_",
    "outputId": "eb72bbe7-16b9-4a85-b6d1-9a1e691a9193"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import hopsworks\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from hsfs.embedding import EmbeddingIndex\n",
    "\n",
    "from functions.zotero_parser import ZoteroCSVParser\n",
    "from functions.PDF_extractor import PDFExtractor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oGPa_qOH0yLQ",
   "metadata": {
    "id": "oGPa_qOH0yLQ"
   },
   "source": [
    "## <span style=\"color:#ff5f27\"> Global Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "S-TH_b7bA8GT",
   "metadata": {
    "id": "S-TH_b7bA8GT"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"HOPSWORKS_API_KEY\"] = \"1QJZ515qO3Hl6pwr.Kr6HwXJ5SbnYV6TeEyAEyDGsV31Is9rryhZUyvRjamJjodvONIodYhBskNcZxHAz\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f783e27e",
   "metadata": {
    "id": "f783e27e"
   },
   "source": [
    "## <span style=\"color:#ff5f27\">üß¨ Metadata and Text Extraction </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "877d820a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import Optional, Dict, Any\n",
    "\n",
    "def sanitize_paper_metadata(paper: Dict[str, Any]) -> Optional[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Defensive metadata sanitation.\n",
    "    Returns None if the paper is considered invalid.\n",
    "    \"\"\"\n",
    "\n",
    "    # ---- 1. Mandatory Field Validation ----\n",
    "    title = paper.get(\"title\", \"\").strip()\n",
    "    if not title:\n",
    "        return None  # Equivalent to RDF version: discard if title is missing\n",
    "\n",
    "    paper[\"title\"] = title\n",
    "\n",
    "    # ---- 2. Year Repair (Reusing regex logic from RDF) ----\n",
    "    year = paper.get(\"year\")\n",
    "    if year is None:\n",
    "        # Attempt to recover year from other metadata fields\n",
    "        for field in (\"url\", \"abstract\"):\n",
    "            text = paper.get(field, \"\")\n",
    "            match = re.search(r\"(19|20)\\d{2}\", text)\n",
    "            if match:\n",
    "                paper[\"year\"] = int(match.group())\n",
    "                break\n",
    "\n",
    "    # ---- 3. Authors Fallback ----\n",
    "    authors = paper.get(\"authors\", \"\").strip()\n",
    "    if not authors or authors.lower() == \"nan\":\n",
    "        paper[\"authors\"] = \"Unknown\"\n",
    "\n",
    "    # ---- 4. Abstract Normalization ----\n",
    "    abstract = paper.get(\"abstract\", \"\").strip()\n",
    "    if abstract.lower() in {\"nan\", \"none\"}:\n",
    "        paper[\"abstract\"] = \"\"\n",
    "\n",
    "    # ---- 5. Attachments Handling ----\n",
    "    # Preserve original state but ensure the value is a string\n",
    "    attachments = paper.get(\"file_attachments\")\n",
    "    paper[\"file_attachments\"] = str(attachments) if attachments is not None else \"\"\n",
    "\n",
    "    return paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "xEaWcXPP4oN8",
   "metadata": {
    "id": "xEaWcXPP4oN8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed 15 papers.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'paper_id': 'CLGNKPIJ',\n",
       "  'title': 'Synthesis of Normal Heart Sounds Using Generative Adversarial Networks and Empirical Wavelet Transform',\n",
       "  'authors': 'Narv√°ez, Pedro; Percybrooks, Winston S.',\n",
       "  'year': 2020,\n",
       "  'abstract': 'Currently, there are many works in the literature focused on the analysis of heart sounds, speciÔ¨Åcally on the development of intelligent systems for the classiÔ¨Åcation of normal and abnormal heart sounds. However, the available heart sound databases are not yet large enough to train generalized machine learning models. Therefore, there is interest in the development of algorithms capable of generating heart sounds that could augment current databases. In this article, we propose a model based on generative adversary networks (GANs) to generate normal synthetic heart sounds. Additionally, a denoising algorithm is implemented using the empirical wavelet transform (EWT), allowing a decrease in the number of epochs and the computational cost that the GAN model requires. A distortion metric (mel‚Äìcepstral distortion) was used to objectively assess the quality of synthetic heart sounds. The proposed method was favorably compared with a mathematical model that is based on the morphology of the phonocardiography (PCG) signal published as the state of the art. Additionally, diÔ¨Äerent heart sound classiÔ¨Åcation models proposed as state-of-the-art were also used to test the performance of such models when the GAN-generated synthetic signals were used as test dataset. In this experiment, good accuracy results were obtained with most of the implemented models, suggesting that the GAN-generated sounds correctly capture the characteristics of natural heart sounds.',\n",
       "  'item_type': 'journalArticle',\n",
       "  'file_attachments': 'C:\\\\Users\\\\alyssa\\\\Zotero\\\\storage\\\\J6CFPY56\\\\Narv√°ezÂíåPercybrooks - 2020 - Synthesis of Normal Heart Sounds Using Generative Adversarial Networks and Empirical Wavelet Transfo.pdf',\n",
       "  'url': 'https://www.mdpi.com/2076-3417/10/19/7003'},\n",
       " {'paper_id': 'R7JAHFY6',\n",
       "  'title': 'ECG Generation Based on Denoising Diffusion Probabilistic Models',\n",
       "  'authors': 'Wang, Zhongyu; Ma, Caiyun; Zhao, Minghui; Zhang, Shuo; Li, Jianqing; Liu, Chengyu',\n",
       "  'year': 2024,\n",
       "  'abstract': 'Arrhythmia diseases seriously damage people‚Äôs life and health, and identifying abnormal points in ECG signals by deep neural networks is an effective method for detecting arrhythmias. However, their accuracy is often limited by the biased data distribution of the training set, and a large number of labeled ECG signals are usually harder to obtain. Therefore, this paper proposes to synthesize virtual heart beat data by denoising diffusion probability model (DDPM) based on the MIT-BIH arrhythmia database to complement the real data. Three different methods for generating heartbeat signals are also used, which are (i) generating heartbeat signals directly, (ii) generating time-frequency maps of heartbeats and transforming them into heartbeat signals, and (iii) generating sub-signals of heartbeats and fusing them into complete heartbeat signals. Regarding the evaluation of the synthesized signals, we compare the advantages and disadvantages of the three heartbeat generation methods by four metrics: DTW, PCC, ED and KLD. The experimental results showed that the optimum values of 4.37, 17.09, 0.972 and 0.0094 were obtained for ED, DTW of method (i) and PCC, KLD of method (iii), respectively.',\n",
       "  'item_type': 'conferencePaper',\n",
       "  'file_attachments': 'C:\\\\Users\\\\alyssa\\\\Zotero\\\\storage\\\\JZPFTQVT\\\\Wang Á≠â - 2024 - ECG Generation Based on Denoising Diffusion Probabilistic Models.pdf',\n",
       "  'url': 'https://www.cinc.org/archives/2024/pdf/CinC2024-027.pdf'}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === Cell 3: Parse Zotero CSV ===\n",
    "parser = ZoteroCSVParser(\"PCG.csv\")\n",
    "raw_papers = parser.parse()\n",
    "\n",
    "papers = []\n",
    "for paper in raw_papers:\n",
    "    fixed = sanitize_paper_metadata(paper)\n",
    "    if fixed is not None:\n",
    "        papers.append(fixed)\n",
    "\n",
    "print(f\"Parsed {len(papers)} papers.\")\n",
    "papers[:2]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "318Tc5AI4t_T",
   "metadata": {
    "id": "318Tc5AI4t_T"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata rows: 15\n",
      "Fulltext chunks: 59\n"
     ]
    }
   ],
   "source": [
    "# === Cell 4: Extract Full Text from Attachments ===\n",
    "\n",
    "import re\n",
    "import urllib.parse\n",
    "from typing import List, Optional\n",
    "\n",
    "from config import MIN_FULLTEXT_LEN, CHUNK_SIZE, CHUNK_OVERLAP\n",
    "\n",
    "\n",
    "metadata_rows = []\n",
    "fulltext_rows = []\n",
    "\n",
    "\n",
    "# -------- Abstract extraction (provided implementation) --------\n",
    "\n",
    "def extract_abstract_from_text(text: str) -> Optional[str]:\n",
    "    normalized = text.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")\n",
    "\n",
    "    stop_markers = (\n",
    "        r\"keywords|index\\s*terms|subject[s]?|introduction|background|materials\\s+and\\s+methods|\"\n",
    "        r\"methods|results|conclusions|references|acknowledg(e)?ments|1\\.|i\\.|ii\\.|iii\\.\"\n",
    "        r\"|Keywords|Introduction|Background|Methods|Results|Conclusion|References\"\n",
    "    )\n",
    "    start_markers = r\"abstract|summary|Abstract|Summary\"\n",
    "\n",
    "    pattern = rf\"(?is)\\b(?:{start_markers})\\b\\s*[:\\.\\-]?\\s*(.+?)(?=\\n\\s*(?:{stop_markers})\\b|\\n\\n\\s*[A-Z][A-Za-z ]+\\b|\\Z)\"\n",
    "    match = re.search(pattern, normalized)\n",
    "    if match:\n",
    "        abstract = re.sub(r\"\\s+\", \" \", match.group(1).strip())\n",
    "        if 50 <= len(abstract) <= 5000 and re.search(r\"[a-z]\", abstract, re.I):\n",
    "            return abstract\n",
    "\n",
    "    lines = normalized.split(\"\\n\")\n",
    "    abstract_started = False\n",
    "    buffer: list[str] = []\n",
    "\n",
    "    for line in lines:\n",
    "        line_stripped = line.strip()\n",
    "\n",
    "        if not abstract_started:\n",
    "            if re.match(\n",
    "                r\"(?i)^(abstract|summary)\\b\\s*[:\\-\\.]?\\s*$\",\n",
    "                line_stripped,\n",
    "            ) or re.match(\n",
    "                r\"(?i)^(abstract|summary)\\b\\s*[:\\-\\.]?\",\n",
    "                line_stripped,\n",
    "            ):\n",
    "                after = re.sub(\n",
    "                    r\"(?i)^(abstract|summary)\\b\\s*[:\\-\\.]?\\s*\",\n",
    "                    \"\",\n",
    "                    line_stripped,\n",
    "                )\n",
    "                if after:\n",
    "                    buffer.append(after)\n",
    "                abstract_started = True\n",
    "            continue\n",
    "        else:\n",
    "            if re.match(rf\"(?i)^\\s*(?:{stop_markers})\\b\", line_stripped):\n",
    "                break\n",
    "            buffer.append(line)\n",
    "\n",
    "    candidate = re.sub(r\"\\s+\", \" \", \" \".join(buffer)).strip()\n",
    "    if 50 <= len(candidate) <= 5000 and re.search(r\"[a-z]\", candidate, re.I):\n",
    "        return candidate\n",
    "\n",
    "    paragraphs = re.split(r\"\\n\\s*\\n\", normalized)\n",
    "    for paragraph in paragraphs[:8]:\n",
    "        p = re.sub(r\"\\s+\", \" \", paragraph.strip())\n",
    "        if (\n",
    "            120 <= len(p) <= 5000\n",
    "            and not re.match(\n",
    "                r\"(?i)^(keywords|index\\s*terms|introduction|references|acknowledg(e)?ments)\",\n",
    "                p,\n",
    "            )\n",
    "            and p.count(\".\") >= 2\n",
    "        ):\n",
    "            return p\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "# -------- Content cleaning --------\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    if not text:\n",
    "        return \"\"\n",
    "\n",
    "    text = re.sub(r\"(\\w+)-\\s*\\n\\s*(\\w+)\", r\"\\1\\2\", text)\n",
    "    text = text.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")\n",
    "    text = re.sub(r\"(\\.{5,}|\\-{5,})\", \" \", text)\n",
    "    text = re.sub(r\"[\\t\\f\\u00A0]+\", \" \", text)\n",
    "    text = re.sub(r\"\\n{3,}\", \"\\n\\n\", text)\n",
    "\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "# -------- Chunking --------\n",
    "\n",
    "def chunk_text(\n",
    "    text: str,\n",
    "    chunk_size: int = CHUNK_SIZE,\n",
    "    overlap: int = CHUNK_OVERLAP,\n",
    ") -> List[str]:\n",
    "    if not text:\n",
    "        return []\n",
    "\n",
    "    paragraphs = text.split(\"\\n\\n\")\n",
    "    chunks = []\n",
    "    current = \"\"\n",
    "\n",
    "    for para in paragraphs:\n",
    "        para = para.strip()\n",
    "        if not para:\n",
    "            continue\n",
    "\n",
    "        if len(current) + len(para) < chunk_size:\n",
    "            current = f\"{current}\\n\\n{para}\" if current else para\n",
    "        else:\n",
    "            if current:\n",
    "                chunks.append(current)\n",
    "                current = current[-overlap:] + \"\\n\\n\" + para\n",
    "\n",
    "            if len(current) > chunk_size:\n",
    "                for i in range(0, len(current), chunk_size - overlap):\n",
    "                    chunks.append(current[i : i + chunk_size])\n",
    "                current = \"\"\n",
    "\n",
    "    if current:\n",
    "        chunks.append(current)\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "# -------- Safe file reading --------\n",
    "\n",
    "def safe_read_fulltext(file_path: str) -> str:\n",
    "    if not file_path:\n",
    "        return \"\"\n",
    "\n",
    "    decoded = urllib.parse.unquote(file_path)\n",
    "    return PDFExtractor.read_file(decoded) or \"\"\n",
    "\n",
    "\n",
    "# -------- Main loop --------\n",
    "\n",
    "for paper in papers:\n",
    "    # ---- Full text extraction ----\n",
    "    raw_full_text = safe_read_fulltext(paper.get(\"file_attachments\", \"\"))\n",
    "\n",
    "    # ---- Content processing ----\n",
    "    full_text = clean_text(raw_full_text)\n",
    "\n",
    "    # ---- Abstract handling ----\n",
    "    abstract = paper.get(\"abstract\", \"\")\n",
    "    if not abstract and len(full_text) >= MIN_FULLTEXT_LEN:\n",
    "        abstract = extract_abstract_from_text(full_text) or \"\"\n",
    "\n",
    "    # ---- Paper-level features ----\n",
    "    metadata_rows.append(\n",
    "        {\n",
    "            \"paper_id\": paper[\"paper_id\"],\n",
    "            \"title\": paper[\"title\"],\n",
    "            \"abstract\": abstract,\n",
    "            \"authors\": paper[\"authors\"],\n",
    "            \"year\": paper[\"year\"],\n",
    "            \"item_type\": paper[\"item_type\"],\n",
    "            \"combined_text\": (\n",
    "                f\"Title: {paper['title']}\\n\"\n",
    "                f\"Abstract: {abstract}\"\n",
    "            ),\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # ---- Chunk-level features ----\n",
    "    if len(full_text) >= MIN_FULLTEXT_LEN:\n",
    "        for i, chunk in enumerate(chunk_text(full_text)):\n",
    "            fulltext_rows.append(\n",
    "                {\n",
    "                    \"paper_id\": paper[\"paper_id\"],\n",
    "                    \"chunk_index\": i,\n",
    "                    \"content\": chunk,\n",
    "                    \"year\": paper[\"year\"],\n",
    "                }\n",
    "            )\n",
    "\n",
    "print(f\"Metadata rows: {len(metadata_rows)}\")\n",
    "print(f\"Fulltext chunks: {len(fulltext_rows)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3b3d28",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27;\"> üîÆ Embedding Extraction </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "862c2315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-08 01:09:41,354 INFO: Use pytorch device_name: cpu\n",
      "2026-01-08 01:09:41,355 INFO: Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "Loaded embedding model: all-MiniLM-L6-v2\n",
      "Embedding dimension: 384\n",
      "Metadata rows: 15\n",
      "Chunk rows: 59\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69d86b72c5e14b179b17a67caa51986c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "Expected a 1D array, got an array with shape (15, 384)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32me:\\anaconda3\\envs\\aq\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3791\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3790\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3791\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3792\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:152\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:181\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'embedding'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32me:\\anaconda3\\envs\\aq\\lib\\site-packages\\pandas\\core\\frame.py:4261\u001b[0m, in \u001b[0;36mDataFrame._set_item_mgr\u001b[1;34m(self, key, value, refs)\u001b[0m\n\u001b[0;32m   4260\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 4261\u001b[0m     loc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_info_axis\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4262\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[0;32m   4263\u001b[0m     \u001b[38;5;66;03m# This item wasn't present, just insert at end\u001b[39;00m\n",
      "File \u001b[1;32me:\\anaconda3\\envs\\aq\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3798\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3797\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3798\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3799\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3800\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3801\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3802\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'embedding'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 37\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# -------------------------\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# 3. Generate metadata embeddings\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# -------------------------\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m df_metadata\u001b[38;5;241m.\u001b[39mempty:\n\u001b[1;32m---> 37\u001b[0m     \u001b[43mdf_metadata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43membedding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mencode(\n\u001b[0;32m     38\u001b[0m         df_metadata[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcombined_text\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist(),\n\u001b[0;32m     39\u001b[0m         show_progress_bar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     40\u001b[0m         convert_to_numpy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     41\u001b[0m     )\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     43\u001b[0m     df_metadata[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membedding\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32me:\\anaconda3\\envs\\aq\\lib\\site-packages\\pandas\\core\\frame.py:4091\u001b[0m, in \u001b[0;36mDataFrame.__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   4088\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setitem_array([key], value)\n\u001b[0;32m   4089\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   4090\u001b[0m     \u001b[38;5;66;03m# set column\u001b[39;00m\n\u001b[1;32m-> 4091\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_set_item\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\anaconda3\\envs\\aq\\lib\\site-packages\\pandas\\core\\frame.py:4314\u001b[0m, in \u001b[0;36mDataFrame._set_item\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   4311\u001b[0m             value \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mtile(value, (\u001b[38;5;28mlen\u001b[39m(existing_piece\u001b[38;5;241m.\u001b[39mcolumns), \u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mT\n\u001b[0;32m   4312\u001b[0m             refs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 4314\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_set_item_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrefs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\anaconda3\\envs\\aq\\lib\\site-packages\\pandas\\core\\frame.py:4264\u001b[0m, in \u001b[0;36mDataFrame._set_item_mgr\u001b[1;34m(self, key, value, refs)\u001b[0m\n\u001b[0;32m   4261\u001b[0m     loc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info_axis\u001b[38;5;241m.\u001b[39mget_loc(key)\n\u001b[0;32m   4262\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[0;32m   4263\u001b[0m     \u001b[38;5;66;03m# This item wasn't present, just insert at end\u001b[39;00m\n\u001b[1;32m-> 4264\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minsert\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_info_axis\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrefs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4265\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   4266\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iset_item_mgr(loc, value, refs\u001b[38;5;241m=\u001b[39mrefs)\n",
      "File \u001b[1;32me:\\anaconda3\\envs\\aq\\lib\\site-packages\\pandas\\core\\internals\\managers.py:1328\u001b[0m, in \u001b[0;36mBlockManager.insert\u001b[1;34m(self, loc, item, value, refs)\u001b[0m\n\u001b[0;32m   1326\u001b[0m     value \u001b[38;5;241m=\u001b[39m value\u001b[38;5;241m.\u001b[39mT\n\u001b[0;32m   1327\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(value) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m-> 1328\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1329\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected a 1D array, got an array with shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;241m.\u001b[39mT\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1330\u001b[0m         )\n\u001b[0;32m   1331\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1332\u001b[0m     value \u001b[38;5;241m=\u001b[39m ensure_block_shape(value, ndim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mndim)\n",
      "\u001b[1;31mValueError\u001b[0m: Expected a 1D array, got an array with shape (15, 384)"
     ]
    }
   ],
   "source": [
    "# === Cell 5: Generate Embeddings for Metadata and Full Text ===\n",
    "\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from hsfs import embedding\n",
    "\n",
    "from config import EMBEDDING_MODEL_NAME\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# 1. Load embedding model\n",
    "# -------------------------\n",
    "\n",
    "model = SentenceTransformer(EMBEDDING_MODEL_NAME)\n",
    "embedding_dim = model.get_sentence_embedding_dimension()\n",
    "\n",
    "print(f\"Loaded embedding model: {EMBEDDING_MODEL_NAME}\")\n",
    "print(f\"Embedding dimension: {embedding_dim}\")\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# 2. Prepare DataFrames\n",
    "# -------------------------\n",
    "\n",
    "df_metadata = pd.DataFrame(metadata_rows)\n",
    "df_chunks = pd.DataFrame(fulltext_rows)\n",
    "\n",
    "print(f\"Metadata rows: {len(df_metadata)}\")\n",
    "print(f\"Chunk rows: {len(df_chunks)}\")\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# 3. Generate metadata embeddings\n",
    "# -------------------------\n",
    "\n",
    "if not df_metadata.empty:\n",
    "    df_metadata[\"embedding\"] = model.encode(\n",
    "        df_metadata[\"combined_text\"].tolist(),\n",
    "        show_progress_bar=True,\n",
    "        convert_to_numpy=True,\n",
    "    )\n",
    "else:\n",
    "    df_metadata[\"embedding\"] = []\n",
    "\n",
    "print(\"Metadata embeddings generated.\")\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# 4. Generate full-text chunk embeddings\n",
    "# -------------------------\n",
    "\n",
    "if not df_chunks.empty:\n",
    "    df_chunks[\"embedding\"] = model.encode(\n",
    "        df_chunks[\"content\"].tolist(),\n",
    "        show_progress_bar=True,\n",
    "        convert_to_numpy=True,\n",
    "    )\n",
    "else:\n",
    "    df_chunks[\"embedding\"] = []\n",
    "\n",
    "print(\"Chunk embeddings generated.\")\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# 5. Add context_id (stable row id)\n",
    "# -------------------------\n",
    "\n",
    "df_metadata[\"context_id\"] = range(len(df_metadata))\n",
    "df_chunks[\"context_id\"] = range(len(df_chunks))\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# 6. Create embedding indexes\n",
    "# -------------------------\n",
    "\n",
    "metadata_index = embedding.EmbeddingIndex()\n",
    "metadata_index.add_embedding(\n",
    "    \"metadata_embedding\",\n",
    "    embedding_dim,\n",
    ")\n",
    "\n",
    "chunk_index = embedding.EmbeddingIndex()\n",
    "chunk_index.add_embedding(\n",
    "    \"chunk_embedding\",\n",
    "    embedding_dim,\n",
    ")\n",
    "\n",
    "print(\"Embedding indexes created.\")\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# 7. Final sanity check\n",
    "# -------------------------\n",
    "\n",
    "display(df_metadata.head())\n",
    "display(df_chunks.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2bced31",
   "metadata": {
    "id": "d2bced31"
   },
   "source": [
    "## <span style=\"color:#ff5f27;\"> üîÆ Connecting to Hopsworks Feature Store </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7caf764d",
   "metadata": {
    "id": "7caf764d"
   },
   "outputs": [],
   "source": [
    "import hopsworks\n",
    "from config import HOPSWORKS_API_KEY\n",
    "# project = hopsworks.login()\n",
    "\n",
    "project = hopsworks.login(\n",
    "        # project=HOPSWORKS_PROJECT,\n",
    "        api_key_value=HOPSWORKS_API_KEY\n",
    "    )\n",
    "\n",
    "fs = project.get_feature_store()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed9ac69",
   "metadata": {
    "id": "0ed9ac69"
   },
   "source": [
    "## <span style=\"color:#ff5f27;\"> ü™Ñ Feature Group Creation </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5e486b",
   "metadata": {
    "id": "9f5e486b"
   },
   "outputs": [],
   "source": [
    "# === Cell 6.1: Create or Get Metadata Feature Group (Safe Version) ===\n",
    "\n",
    "from hsfs import embedding\n",
    "\n",
    "metadata_emb_index = embedding.EmbeddingIndex()\n",
    "metadata_emb_index.add_embedding(\n",
    "    \"embedding\",\n",
    "    model.get_sentence_embedding_dimension(),\n",
    ")\n",
    "\n",
    "metadata_fg = fs.get_or_create_feature_group(\n",
    "    name=\"paper_metadata_fg\",\n",
    "    version=1,\n",
    "    description=\"Paper-level metadata features (title, abstract, authors, year, item type)\",\n",
    "    primary_key=[\"paper_id\"],\n",
    "    online_enabled=False,\n",
    "    embedding_index=metadata_emb_index,\n",
    ")\n",
    "\n",
    "\n",
    "metadata_fg.insert(\n",
    "    df_metadata,\n",
    "    write_options={\"wait_for_job\": True},\n",
    ")\n",
    "\n",
    "print(f\"Inserted {len(df_metadata)} rows into metadata feature group.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e32b548",
   "metadata": {
    "id": "6e32b548"
   },
   "outputs": [],
   "source": [
    "# === Cell 6.2: Create or Get Fulltext Chunk Feature Group (Safe Version) ===\n",
    "\n",
    "chunk_emb_index = embedding.EmbeddingIndex()\n",
    "chunk_emb_index.add_embedding(\n",
    "    \"embedding\",\n",
    "    model.get_sentence_embedding_dimension(),\n",
    ")\n",
    "\n",
    "chunk_fg = fs.get_or_create_feature_group(\n",
    "    name=\"paper_chunk_fg\",\n",
    "    version=1,\n",
    "    description=\"Chunk-level full text features for RAG\",\n",
    "    primary_key=[\"paper_id\", \"chunk_index\"],\n",
    "    online_enabled=False,\n",
    "    embedding_index=chunk_emb_index,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2133dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_fg.insert(\n",
    "    df_chunks,\n",
    "    write_options={\"wait_for_job\": True},\n",
    ")\n",
    "\n",
    "print(f\"Inserted {len(df_chunks)} rows into chunk feature group.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39a9ed6",
   "metadata": {
    "id": "d39a9ed6"
   },
   "source": [
    "## <span style=\"color:#ff5f27;\">ü™Ñ Feature View Creation </span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "OJ6koRqW6KsN",
   "metadata": {
    "id": "OJ6koRqW6KsN"
   },
   "outputs": [],
   "source": [
    "# === Cell 8.1: Create Metadata Feature View ===\n",
    "\n",
    "metadata_fv = fs.get_or_create_feature_view(\n",
    "    name=\"paper_metadata_fv\",\n",
    "    version=1,\n",
    "    description=\"Paper-level metadata for retrieval and filtering\",\n",
    "    query=metadata_fg.select(\n",
    "        [\n",
    "            \"paper_id\",\n",
    "            \"title\",\n",
    "            \"abstract\",\n",
    "            \"authors\",\n",
    "            \"year\",\n",
    "            \"item_type\",\n",
    "            \"embedding\",\n",
    "        ]\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(\"Metadata Feature View ready.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Bj3WyVOX6MQS",
   "metadata": {
    "id": "Bj3WyVOX6MQS"
   },
   "outputs": [],
   "source": [
    "# === Cell 8.2: Create Chunk Feature View ===\n",
    "\n",
    "chunk_fv = fs.get_or_create_feature_view(\n",
    "    name=\"paper_chunk_fv\",\n",
    "    version=1,\n",
    "    description=\"Chunk-level full text for RAG context retrieval\",\n",
    "    query=chunk_fg.select(\n",
    "        [\n",
    "            \"paper_id\",\n",
    "            \"chunk_index\",\n",
    "            \"content\",\n",
    "            \"year\",\n",
    "            \"embedding\",\n",
    "        ]\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(\"Chunk Feature View ready.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708b9a5f",
   "metadata": {
    "id": "708b9a5f"
   },
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "https://github.com/alyssa2024/ID2223-Project/blob/main/1_feature_backfill.ipynb",
     "timestamp": 1767715450010
    }
   ]
  },
  "kernelspec": {
   "display_name": "aq",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
