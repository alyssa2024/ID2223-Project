{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82622ee3",
   "metadata": {
    "id": "82622ee3"
   },
   "source": [
    "## <span style=\"color:#ff5f27\">üìù Imports </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kkvpsHNjyrra",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 34783,
     "status": "ok",
     "timestamp": 1767717940875,
     "user": {
      "displayName": "alyssa",
      "userId": "18151921927230202476"
     },
     "user_tz": -60
    },
    "id": "kkvpsHNjyrra",
    "outputId": "49355b47-0bb8-463f-ac0f-3c87f0344ba3"
   },
   "outputs": [],
   "source": [
    "!pip install -q hopsworks rdflib sentence-transformers pymupdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gCEqcKWw3nb_",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 426
    },
    "executionInfo": {
     "elapsed": 757,
     "status": "error",
     "timestamp": 1767715315868,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -60
    },
    "id": "gCEqcKWw3nb_",
    "outputId": "eb72bbe7-16b9-4a85-b6d1-9a1e691a9193"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import hopsworks\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from hsfs.embedding import EmbeddingIndex\n",
    "\n",
    "from functions.zotero_parser import ZoteroCSVParser\n",
    "from functions.PDF_extractor import PDFExtractor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oGPa_qOH0yLQ",
   "metadata": {
    "id": "oGPa_qOH0yLQ"
   },
   "source": [
    "## <span style=\"color:#ff5f27\"> Global Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "S-TH_b7bA8GT",
   "metadata": {
    "id": "S-TH_b7bA8GT"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"HOPSWORKS_API_KEY\"] = \"1QJZ515qO3Hl6pwr.Kr6HwXJ5SbnYV6TeEyAEyDGsV31Is9rryhZUyvRjamJjodvONIodYhBskNcZxHAz\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f783e27e",
   "metadata": {
    "id": "f783e27e"
   },
   "source": [
    "## <span style=\"color:#ff5f27\">üß¨ Metadata and Text Extraction, Embedding Creation </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877d820a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import Optional, Dict, Any\n",
    "\n",
    "def sanitize_paper_metadata(paper: Dict[str, Any]) -> Optional[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Defensive metadata sanitation.\n",
    "    Returns None if the paper is considered invalid.\n",
    "    \"\"\"\n",
    "\n",
    "    # ---- 1. Mandatory Field Validation ----\n",
    "    title = paper.get(\"title\", \"\").strip()\n",
    "    if not title:\n",
    "        return None  # Equivalent to RDF version: discard if title is missing\n",
    "\n",
    "    paper[\"title\"] = title\n",
    "\n",
    "    # ---- 2. Year Repair (Reusing regex logic from RDF) ----\n",
    "    year = paper.get(\"year\")\n",
    "    if year is None:\n",
    "        # Attempt to recover year from other metadata fields\n",
    "        for field in (\"url\", \"abstract\"):\n",
    "            text = paper.get(field, \"\")\n",
    "            match = re.search(r\"(19|20)\\d{2}\", text)\n",
    "            if match:\n",
    "                paper[\"year\"] = int(match.group())\n",
    "                break\n",
    "\n",
    "    # ---- 3. Authors Fallback ----\n",
    "    authors = paper.get(\"authors\", \"\").strip()\n",
    "    if not authors or authors.lower() == \"nan\":\n",
    "        paper[\"authors\"] = \"Unknown\"\n",
    "\n",
    "    # ---- 4. Abstract Normalization ----\n",
    "    abstract = paper.get(\"abstract\", \"\").strip()\n",
    "    if abstract.lower() in {\"nan\", \"none\"}:\n",
    "        paper[\"abstract\"] = \"\"\n",
    "\n",
    "    # ---- 5. Attachments Handling ----\n",
    "    # Preserve original state but ensure the value is a string\n",
    "    attachments = paper.get(\"file_attachments\")\n",
    "    paper[\"file_attachments\"] = str(attachments) if attachments is not None else \"\"\n",
    "\n",
    "    return paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xEaWcXPP4oN8",
   "metadata": {
    "id": "xEaWcXPP4oN8"
   },
   "outputs": [],
   "source": [
    "# === Cell 3: Parse Zotero CSV ===\n",
    "parser = ZoteroCSVParser(\"PCG.csv\")\n",
    "raw_papers = parser.parse()\n",
    "\n",
    "papers = []\n",
    "for paper in raw_papers:\n",
    "    fixed = sanitize_paper_metadata(paper)\n",
    "    if fixed is not None:\n",
    "        papers.append(fixed)\n",
    "\n",
    "print(f\"Parsed {len(papers)} papers.\")\n",
    "papers[:2]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318Tc5AI4t_T",
   "metadata": {
    "id": "318Tc5AI4t_T"
   },
   "outputs": [],
   "source": [
    "# === Cell 4: Extract Full Text from Attachments ===\n",
    "\n",
    "import re\n",
    "import urllib.parse\n",
    "from typing import List, Optional\n",
    "\n",
    "from config import MIN_FULLTEXT_LEN, CHUNK_SIZE, CHUNK_OVERLAP\n",
    "\n",
    "\n",
    "metadata_rows = []\n",
    "fulltext_rows = []\n",
    "\n",
    "\n",
    "# -------- Abstract extraction (provided implementation) --------\n",
    "\n",
    "def extract_abstract_from_text(text: str) -> Optional[str]:\n",
    "    normalized = text.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")\n",
    "\n",
    "    stop_markers = (\n",
    "        r\"keywords|index\\s*terms|subject[s]?|introduction|background|materials\\s+and\\s+methods|\"\n",
    "        r\"methods|results|conclusions|references|acknowledg(e)?ments|1\\.|i\\.|ii\\.|iii\\.\"\n",
    "        r\"|Keywords|Introduction|Background|Methods|Results|Conclusion|References\"\n",
    "    )\n",
    "    start_markers = r\"abstract|summary|Abstract|Summary\"\n",
    "\n",
    "    pattern = rf\"(?is)\\b(?:{start_markers})\\b\\s*[:\\.\\-]?\\s*(.+?)(?=\\n\\s*(?:{stop_markers})\\b|\\n\\n\\s*[A-Z][A-Za-z ]+\\b|\\Z)\"\n",
    "    match = re.search(pattern, normalized)\n",
    "    if match:\n",
    "        abstract = re.sub(r\"\\s+\", \" \", match.group(1).strip())\n",
    "        if 50 <= len(abstract) <= 5000 and re.search(r\"[a-z]\", abstract, re.I):\n",
    "            return abstract\n",
    "\n",
    "    lines = normalized.split(\"\\n\")\n",
    "    abstract_started = False\n",
    "    buffer: list[str] = []\n",
    "\n",
    "    for line in lines:\n",
    "        line_stripped = line.strip()\n",
    "\n",
    "        if not abstract_started:\n",
    "            if re.match(\n",
    "                r\"(?i)^(abstract|summary)\\b\\s*[:\\-\\.]?\\s*$\",\n",
    "                line_stripped,\n",
    "            ) or re.match(\n",
    "                r\"(?i)^(abstract|summary)\\b\\s*[:\\-\\.]?\",\n",
    "                line_stripped,\n",
    "            ):\n",
    "                after = re.sub(\n",
    "                    r\"(?i)^(abstract|summary)\\b\\s*[:\\-\\.]?\\s*\",\n",
    "                    \"\",\n",
    "                    line_stripped,\n",
    "                )\n",
    "                if after:\n",
    "                    buffer.append(after)\n",
    "                abstract_started = True\n",
    "            continue\n",
    "        else:\n",
    "            if re.match(rf\"(?i)^\\s*(?:{stop_markers})\\b\", line_stripped):\n",
    "                break\n",
    "            buffer.append(line)\n",
    "\n",
    "    candidate = re.sub(r\"\\s+\", \" \", \" \".join(buffer)).strip()\n",
    "    if 50 <= len(candidate) <= 5000 and re.search(r\"[a-z]\", candidate, re.I):\n",
    "        return candidate\n",
    "\n",
    "    paragraphs = re.split(r\"\\n\\s*\\n\", normalized)\n",
    "    for paragraph in paragraphs[:8]:\n",
    "        p = re.sub(r\"\\s+\", \" \", paragraph.strip())\n",
    "        if (\n",
    "            120 <= len(p) <= 5000\n",
    "            and not re.match(\n",
    "                r\"(?i)^(keywords|index\\s*terms|introduction|references|acknowledg(e)?ments)\",\n",
    "                p,\n",
    "            )\n",
    "            and p.count(\".\") >= 2\n",
    "        ):\n",
    "            return p\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "# -------- Content cleaning --------\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    if not text:\n",
    "        return \"\"\n",
    "\n",
    "    text = re.sub(r\"(\\w+)-\\s*\\n\\s*(\\w+)\", r\"\\1\\2\", text)\n",
    "    text = text.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")\n",
    "    text = re.sub(r\"(\\.{5,}|\\-{5,})\", \" \", text)\n",
    "    text = re.sub(r\"[\\t\\f\\u00A0]+\", \" \", text)\n",
    "    text = re.sub(r\"\\n{3,}\", \"\\n\\n\", text)\n",
    "\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "# -------- Chunking --------\n",
    "\n",
    "def chunk_text(\n",
    "    text: str,\n",
    "    chunk_size: int = CHUNK_SIZE,\n",
    "    overlap: int = CHUNK_OVERLAP,\n",
    ") -> List[str]:\n",
    "    if not text:\n",
    "        return []\n",
    "\n",
    "    paragraphs = text.split(\"\\n\\n\")\n",
    "    chunks = []\n",
    "    current = \"\"\n",
    "\n",
    "    for para in paragraphs:\n",
    "        para = para.strip()\n",
    "        if not para:\n",
    "            continue\n",
    "\n",
    "        if len(current) + len(para) < chunk_size:\n",
    "            current = f\"{current}\\n\\n{para}\" if current else para\n",
    "        else:\n",
    "            if current:\n",
    "                chunks.append(current)\n",
    "                current = current[-overlap:] + \"\\n\\n\" + para\n",
    "\n",
    "            if len(current) > chunk_size:\n",
    "                for i in range(0, len(current), chunk_size - overlap):\n",
    "                    chunks.append(current[i : i + chunk_size])\n",
    "                current = \"\"\n",
    "\n",
    "    if current:\n",
    "        chunks.append(current)\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "# -------- Safe file reading --------\n",
    "\n",
    "def safe_read_fulltext(file_path: str) -> str:\n",
    "    if not file_path:\n",
    "        return \"\"\n",
    "\n",
    "    decoded = urllib.parse.unquote(file_path)\n",
    "    return PDFExtractor.read_file(decoded) or \"\"\n",
    "\n",
    "\n",
    "# -------- Main loop --------\n",
    "\n",
    "for paper in papers:\n",
    "    # ---- Full text extraction ----\n",
    "    raw_full_text = safe_read_fulltext(paper.get(\"file_attachments\", \"\"))\n",
    "\n",
    "    # ---- Content processing ----\n",
    "    full_text = clean_text(raw_full_text)\n",
    "\n",
    "    # ---- Abstract handling ----\n",
    "    abstract = paper.get(\"abstract\", \"\")\n",
    "    if not abstract and len(full_text) >= MIN_FULLTEXT_LEN:\n",
    "        abstract = extract_abstract_from_text(full_text) or \"\"\n",
    "\n",
    "    # ---- Paper-level features ----\n",
    "    metadata_rows.append(\n",
    "        {\n",
    "            \"paper_id\": paper[\"paper_id\"],\n",
    "            \"title\": paper[\"title\"],\n",
    "            \"abstract\": abstract,\n",
    "            \"authors\": paper[\"authors\"],\n",
    "            \"year\": paper[\"year\"],\n",
    "            \"item_type\": paper[\"item_type\"],\n",
    "            \"combined_text\": (\n",
    "                f\"Title: {paper['title']}\\n\"\n",
    "                f\"Abstract: {abstract}\"\n",
    "            ),\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # ---- Chunk-level features ----\n",
    "    if len(full_text) >= MIN_FULLTEXT_LEN:\n",
    "        for i, chunk in enumerate(chunk_text(full_text)):\n",
    "            fulltext_rows.append(\n",
    "                {\n",
    "                    \"paper_id\": paper[\"paper_id\"],\n",
    "                    \"chunk_index\": i,\n",
    "                    \"content\": chunk,\n",
    "                    \"year\": paper[\"year\"],\n",
    "                }\n",
    "            )\n",
    "\n",
    "print(f\"Metadata rows: {len(metadata_rows)}\")\n",
    "print(f\"Fulltext chunks: {len(fulltext_rows)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862c2315",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 5: Generate Embeddings for Metadata and Full Text ===\n",
    "\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from hsfs import embedding\n",
    "\n",
    "from config import EMBEDDING_MODEL_NAME\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# 1. Load embedding model\n",
    "# -------------------------\n",
    "\n",
    "model = SentenceTransformer(EMBEDDING_MODEL_NAME)\n",
    "embedding_dim = model.get_sentence_embedding_dimension()\n",
    "\n",
    "print(f\"Loaded embedding model: {EMBEDDING_MODEL_NAME}\")\n",
    "print(f\"Embedding dimension: {embedding_dim}\")\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# 2. Prepare DataFrames\n",
    "# -------------------------\n",
    "\n",
    "df_metadata = pd.DataFrame(metadata_rows)\n",
    "df_chunks = pd.DataFrame(fulltext_rows)\n",
    "\n",
    "print(f\"Metadata rows: {len(df_metadata)}\")\n",
    "print(f\"Chunk rows: {len(df_chunks)}\")\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# 3. Generate metadata embeddings\n",
    "# -------------------------\n",
    "\n",
    "if not df_metadata.empty:\n",
    "    df_metadata[\"embedding\"] = model.encode(\n",
    "        df_metadata[\"combined_text\"].tolist(),\n",
    "        show_progress_bar=True,\n",
    "        convert_to_numpy=True,\n",
    "    )\n",
    "else:\n",
    "    df_metadata[\"embedding\"] = []\n",
    "\n",
    "print(\"Metadata embeddings generated.\")\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# 4. Generate full-text chunk embeddings\n",
    "# -------------------------\n",
    "\n",
    "if not df_chunks.empty:\n",
    "    df_chunks[\"embedding\"] = model.encode(\n",
    "        df_chunks[\"content\"].tolist(),\n",
    "        show_progress_bar=True,\n",
    "        convert_to_numpy=True,\n",
    "    )\n",
    "else:\n",
    "    df_chunks[\"embedding\"] = []\n",
    "\n",
    "print(\"Chunk embeddings generated.\")\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# 5. Add context_id (stable row id)\n",
    "# -------------------------\n",
    "\n",
    "df_metadata[\"context_id\"] = range(len(df_metadata))\n",
    "df_chunks[\"context_id\"] = range(len(df_chunks))\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# 6. Create embedding indexes\n",
    "# -------------------------\n",
    "\n",
    "metadata_index = embedding.EmbeddingIndex()\n",
    "metadata_index.add_embedding(\n",
    "    \"metadata_embedding\",\n",
    "    embedding_dim,\n",
    ")\n",
    "\n",
    "chunk_index = embedding.EmbeddingIndex()\n",
    "chunk_index.add_embedding(\n",
    "    \"chunk_embedding\",\n",
    "    embedding_dim,\n",
    ")\n",
    "\n",
    "print(\"Embedding indexes created.\")\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# 7. Final sanity check\n",
    "# -------------------------\n",
    "\n",
    "display(df_metadata.head())\n",
    "display(df_chunks.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2bced31",
   "metadata": {
    "id": "d2bced31"
   },
   "source": [
    "## <span style=\"color:#ff5f27;\"> üîÆ Connecting to Hopsworks Feature Store </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7caf764d",
   "metadata": {
    "id": "7caf764d"
   },
   "outputs": [],
   "source": [
    "import hopsworks\n",
    "from config import HOPSWORKS_API_KEY\n",
    "# project = hopsworks.login()\n",
    "\n",
    "project = hopsworks.login(\n",
    "        # project=HOPSWORKS_PROJECT,\n",
    "        api_key_value=HOPSWORKS_API_KEY\n",
    "    )\n",
    "\n",
    "fs = project.get_feature_store()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed9ac69",
   "metadata": {
    "id": "0ed9ac69"
   },
   "source": [
    "## <span style=\"color:#ff5f27;\"> ü™Ñ Feature Group Creation </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5e486b",
   "metadata": {
    "id": "9f5e486b"
   },
   "outputs": [],
   "source": [
    "# === Cell 6.1: Create or Get Metadata Feature Group (Safe Version) ===\n",
    "\n",
    "from hsfs import embedding\n",
    "\n",
    "metadata_emb_index = embedding.EmbeddingIndex()\n",
    "metadata_emb_index.add_embedding(\n",
    "    \"embedding\",\n",
    "    model.get_sentence_embedding_dimension(),\n",
    ")\n",
    "\n",
    "metadata_fg = fs.get_or_create_feature_group(\n",
    "    name=\"paper_metadata_fg\",\n",
    "    version=1,\n",
    "    description=\"Paper-level metadata features (title, abstract, authors, year, item type)\",\n",
    "    primary_key=[\"paper_id\"],\n",
    "    online_enabled=False,\n",
    "    embedding_index=metadata_emb_index,\n",
    ")\n",
    "\n",
    "\n",
    "metadata_fg.insert(\n",
    "    df_metadata,\n",
    "    write_options={\"wait_for_job\": True},\n",
    ")\n",
    "\n",
    "print(f\"Inserted {len(df_metadata)} rows into metadata feature group.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e32b548",
   "metadata": {
    "id": "6e32b548"
   },
   "outputs": [],
   "source": [
    "# === Cell 6.2: Create or Get Fulltext Chunk Feature Group (Safe Version) ===\n",
    "\n",
    "chunk_emb_index = embedding.EmbeddingIndex()\n",
    "chunk_emb_index.add_embedding(\n",
    "    \"embedding\",\n",
    "    model.get_sentence_embedding_dimension(),\n",
    ")\n",
    "\n",
    "chunk_fg = fs.get_or_create_feature_group(\n",
    "    name=\"paper_chunk_fg\",\n",
    "    version=1,\n",
    "    description=\"Chunk-level full text features for RAG\",\n",
    "    primary_key=[\"paper_id\", \"chunk_index\"],\n",
    "    online_enabled=False,\n",
    "    embedding_index=chunk_emb_index,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2133dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_fg.insert(\n",
    "    df_chunks,\n",
    "    write_options={\"wait_for_job\": True},\n",
    ")\n",
    "\n",
    "print(f\"Inserted {len(df_chunks)} rows into chunk feature group.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39a9ed6",
   "metadata": {
    "id": "d39a9ed6"
   },
   "source": [
    "## <span style=\"color:#ff5f27;\">ü™Ñ Feature View Creation </span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "OJ6koRqW6KsN",
   "metadata": {
    "id": "OJ6koRqW6KsN"
   },
   "outputs": [],
   "source": [
    "# === Cell 8.1: Create Metadata Feature View ===\n",
    "\n",
    "metadata_fv = fs.get_or_create_feature_view(\n",
    "    name=\"paper_metadata_fv\",\n",
    "    version=1,\n",
    "    description=\"Paper-level metadata for retrieval and filtering\",\n",
    "    query=metadata_fg.select(\n",
    "        [\n",
    "            \"paper_id\",\n",
    "            \"title\",\n",
    "            \"abstract\",\n",
    "            \"authors\",\n",
    "            \"year\",\n",
    "            \"item_type\",\n",
    "            \"embedding\",\n",
    "        ]\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(\"Metadata Feature View ready.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Bj3WyVOX6MQS",
   "metadata": {
    "id": "Bj3WyVOX6MQS"
   },
   "outputs": [],
   "source": [
    "# === Cell 8.2: Create Chunk Feature View ===\n",
    "\n",
    "chunk_fv = fs.get_or_create_feature_view(\n",
    "    name=\"paper_chunk_fv\",\n",
    "    version=1,\n",
    "    description=\"Chunk-level full text for RAG context retrieval\",\n",
    "    query=chunk_fg.select(\n",
    "        [\n",
    "            \"paper_id\",\n",
    "            \"chunk_index\",\n",
    "            \"content\",\n",
    "            \"year\",\n",
    "            \"embedding\",\n",
    "        ]\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(\"Chunk Feature View ready.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708b9a5f",
   "metadata": {
    "id": "708b9a5f"
   },
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "https://github.com/alyssa2024/ID2223-Project/blob/main/1_feature_backfill.ipynb",
     "timestamp": 1767715450010
    }
   ]
  },
  "kernelspec": {
   "display_name": "llm-agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
