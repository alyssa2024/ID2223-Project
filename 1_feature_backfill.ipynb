{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82622ee3",
   "metadata": {
    "id": "82622ee3"
   },
   "source": [
    "## <span style=\"color:#ff5f27\">üìù Imports </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kkvpsHNjyrra",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 34783,
     "status": "ok",
     "timestamp": 1767717940875,
     "user": {
      "displayName": "alyssa",
      "userId": "18151921927230202476"
     },
     "user_tz": -60
    },
    "id": "kkvpsHNjyrra",
    "outputId": "49355b47-0bb8-463f-ac0f-3c87f0344ba3"
   },
   "outputs": [],
   "source": [
    "!pip install -q hopsworks==4.2.10 rdflib sentence-transformers pymupdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "gCEqcKWw3nb_",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 426
    },
    "executionInfo": {
     "elapsed": 757,
     "status": "error",
     "timestamp": 1767715315868,
     "user": {
      "displayName": "",
      "userId": ""
     },
     "user_tz": -60
    },
    "id": "gCEqcKWw3nb_",
    "outputId": "eb72bbe7-16b9-4a85-b6d1-9a1e691a9193"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import hopsworks\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from hsfs.embedding import EmbeddingIndex\n",
    "\n",
    "from functions.zotero_parser import ZoteroCSVParser\n",
    "from functions.PDF_extractor import PDFExtractor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oGPa_qOH0yLQ",
   "metadata": {
    "id": "oGPa_qOH0yLQ"
   },
   "source": [
    "## <span style=\"color:#ff5f27\"> Global Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "S-TH_b7bA8GT",
   "metadata": {
    "id": "S-TH_b7bA8GT"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"HOPSWORKS_API_KEY\"] = \"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f783e27e",
   "metadata": {
    "id": "f783e27e"
   },
   "source": [
    "## <span style=\"color:#ff5f27\">üß¨ Metadata and Text Extraction </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xEaWcXPP4oN8",
   "metadata": {
    "id": "xEaWcXPP4oN8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed 15 papers.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'paper_id': 'CLGNKPIJ',\n",
       "  'title': 'Synthesis of Normal Heart Sounds Using Generative Adversarial Networks and Empirical Wavelet Transform',\n",
       "  'authors': 'Narv√°ez, Pedro; Percybrooks, Winston S.',\n",
       "  'year': 2020,\n",
       "  'abstract': 'Currently, there are many works in the literature focused on the analysis of heart sounds, speciÔ¨Åcally on the development of intelligent systems for the classiÔ¨Åcation of normal and abnormal heart sounds. However, the available heart sound databases are not yet large enough to train generalized machine learning models. Therefore, there is interest in the development of algorithms capable of generating heart sounds that could augment current databases. In this article, we propose a model based on generative adversary networks (GANs) to generate normal synthetic heart sounds. Additionally, a denoising algorithm is implemented using the empirical wavelet transform (EWT), allowing a decrease in the number of epochs and the computational cost that the GAN model requires. A distortion metric (mel‚Äìcepstral distortion) was used to objectively assess the quality of synthetic heart sounds. The proposed method was favorably compared with a mathematical model that is based on the morphology of the phonocardiography (PCG) signal published as the state of the art. Additionally, diÔ¨Äerent heart sound classiÔ¨Åcation models proposed as state-of-the-art were also used to test the performance of such models when the GAN-generated synthetic signals were used as test dataset. In this experiment, good accuracy results were obtained with most of the implemented models, suggesting that the GAN-generated sounds correctly capture the characteristics of natural heart sounds.',\n",
       "  'item_type': 'journalArticle',\n",
       "  'file_attachments': 'C:\\\\Users\\\\alyssa\\\\Zotero\\\\storage\\\\J6CFPY56\\\\Narv√°ezÂíåPercybrooks - 2020 - Synthesis of Normal Heart Sounds Using Generative Adversarial Networks and Empirical Wavelet Transfo.pdf',\n",
       "  'url': 'https://www.mdpi.com/2076-3417/10/19/7003'},\n",
       " {'paper_id': 'R7JAHFY6',\n",
       "  'title': 'ECG Generation Based on Denoising Diffusion Probabilistic Models',\n",
       "  'authors': 'Wang, Zhongyu; Ma, Caiyun; Zhao, Minghui; Zhang, Shuo; Li, Jianqing; Liu, Chengyu',\n",
       "  'year': 2024,\n",
       "  'abstract': 'Arrhythmia diseases seriously damage people‚Äôs life and health, and identifying abnormal points in ECG signals by deep neural networks is an effective method for detecting arrhythmias. However, their accuracy is often limited by the biased data distribution of the training set, and a large number of labeled ECG signals are usually harder to obtain. Therefore, this paper proposes to synthesize virtual heart beat data by denoising diffusion probability model (DDPM) based on the MIT-BIH arrhythmia database to complement the real data. Three different methods for generating heartbeat signals are also used, which are (i) generating heartbeat signals directly, (ii) generating time-frequency maps of heartbeats and transforming them into heartbeat signals, and (iii) generating sub-signals of heartbeats and fusing them into complete heartbeat signals. Regarding the evaluation of the synthesized signals, we compare the advantages and disadvantages of the three heartbeat generation methods by four metrics: DTW, PCC, ED and KLD. The experimental results showed that the optimum values of 4.37, 17.09, 0.972 and 0.0094 were obtained for ED, DTW of method (i) and PCC, KLD of method (iii), respectively.',\n",
       "  'item_type': 'conferencePaper',\n",
       "  'file_attachments': 'C:\\\\Users\\\\alyssa\\\\Zotero\\\\storage\\\\JZPFTQVT\\\\Wang Á≠â - 2024 - ECG Generation Based on Denoising Diffusion Probabilistic Models.pdf',\n",
       "  'url': 'https://www.cinc.org/archives/2024/pdf/CinC2024-027.pdf'}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from functions.metadata_check import sanitize_paper_metadata\n",
    "\n",
    "# === Cell 3: Parse Zotero CSV ===\n",
    "parser = ZoteroCSVParser(\"PCG.csv\")\n",
    "raw_papers = parser.parse()\n",
    "\n",
    "papers = []\n",
    "for paper in raw_papers:\n",
    "    fixed = sanitize_paper_metadata(paper)\n",
    "    if fixed is not None:\n",
    "        papers.append(fixed)\n",
    "\n",
    "print(f\"Parsed {len(papers)} papers.\")\n",
    "papers[:2]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "105adf0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- Safe file reading --------\n",
    "import urllib.parse\n",
    "\n",
    "def safe_read_fulltext(file_path: str) -> str:\n",
    "    if not file_path:\n",
    "        return \"\"\n",
    "\n",
    "    decoded = urllib.parse.unquote(file_path)\n",
    "    return PDFExtractor.read_file(decoded) or \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ea47ffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata rows: 15\n",
      "Fulltext chunks: 516\n"
     ]
    }
   ],
   "source": [
    "from functions.test_and_file_processor import clean_text, extract_abstract_from_text, extract_paragraph_chunks\n",
    "from config import MIN_FULLTEXT_LEN\n",
    "\n",
    "# -------- Main loop --------\n",
    "metadata_rows = []\n",
    "fulltext_rows = []\n",
    "\n",
    "for paper in papers:\n",
    "    # ---- Full text extraction ----\n",
    "    raw_full_text = safe_read_fulltext(paper.get(\"file_attachments\", \"\"))\n",
    "\n",
    "    # ---- Content processing ----\n",
    "    full_text = clean_text(raw_full_text)\n",
    "\n",
    "    # ---- Abstract handling ----\n",
    "    abstract = paper.get(\"abstract\", \"\")\n",
    "    if not abstract and len(full_text) >= MIN_FULLTEXT_LEN:\n",
    "        abstract = extract_abstract_from_text(full_text) or \"\"\n",
    "\n",
    "    # ---- Paper-level features ----\n",
    "    metadata_rows.append(\n",
    "        {\n",
    "            \"paper_id\": paper[\"paper_id\"],\n",
    "            \"title\": paper[\"title\"],\n",
    "            \"abstract\": abstract,\n",
    "            \"authors\": paper[\"authors\"],\n",
    "            \"year\": paper[\"year\"],\n",
    "            \"item_type\": paper[\"item_type\"],\n",
    "            \"combined_text\": (\n",
    "                f\"Title: {paper['title']}\\n\"\n",
    "                f\"Abstract: {abstract}\"\n",
    "            ),\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # ---- Chunk-level features ----\n",
    "    if len(full_text) >= MIN_FULLTEXT_LEN:\n",
    "        for i, chunk in enumerate(extract_paragraph_chunks(full_text)):\n",
    "            fulltext_rows.append(\n",
    "                {\n",
    "                    \"paper_id\": paper[\"paper_id\"],\n",
    "                    \"chunk_index\": i,\n",
    "                    \"content\": chunk,\n",
    "                    \"year\": paper[\"year\"],\n",
    "                }\n",
    "            )\n",
    "\n",
    "print(f\"Metadata rows: {len(metadata_rows)}\")\n",
    "print(f\"Fulltext chunks: {len(fulltext_rows)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3b3d28",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27;\"> üîÆ Embedding Extraction </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862c2315",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 5: Generate Embeddings for Metadata and Full Text ===\n",
    "\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from hsfs import embedding\n",
    "\n",
    "from config import EMBEDDING_MODEL_NAME\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# 1. Load embedding model\n",
    "# -------------------------\n",
    "\n",
    "model = SentenceTransformer(EMBEDDING_MODEL_NAME)\n",
    "embedding_dim = model.get_sentence_embedding_dimension()\n",
    "\n",
    "print(f\"Loaded embedding model: {EMBEDDING_MODEL_NAME}\")\n",
    "print(f\"Embedding dimension: {embedding_dim}\")\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# 2. Prepare DataFrames\n",
    "# -------------------------\n",
    "\n",
    "df_metadata = pd.DataFrame(metadata_rows)\n",
    "df_chunks = pd.DataFrame(fulltext_rows)\n",
    "\n",
    "print(f\"Metadata rows: {len(df_metadata)}\")\n",
    "print(f\"Chunk rows: {len(df_chunks)}\")\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# 3. Generate metadata embeddings\n",
    "# -------------------------\n",
    "\n",
    "if not df_metadata.empty:\n",
    "    embeddings = model.encode(\n",
    "        df_metadata[\"combined_text\"].tolist(),\n",
    "        show_progress_bar=True,\n",
    "        convert_to_numpy=True,\n",
    "    ).astype(\"float32\")\n",
    "\n",
    "    df_metadata[\"embedding\"] = list(embeddings)\n",
    "else:\n",
    "    df_metadata[\"embedding\"] = []\n",
    "\n",
    "print(\"Metadata embeddings generated.\")\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# 4. Generate full-text chunk embeddings\n",
    "# -------------------------\n",
    "\n",
    "if not df_chunks.empty:\n",
    "    embeddings = model.encode(\n",
    "        df_chunks[\"content\"].fillna(\"\").tolist(),\n",
    "        show_progress_bar=True,\n",
    "        convert_to_numpy=True,\n",
    "    ).astype(\"float32\")\n",
    "\n",
    "    df_chunks[\"embedding\"] = list(embeddings)\n",
    "\n",
    "else:\n",
    "    df_chunks[\"embedding\"] = []\n",
    "\n",
    "print(\"Chunk embeddings generated.\")\n",
    "\n",
    "\n",
    "# # -------------------------\n",
    "# # 5. Add context_id (stable row id)\n",
    "# # -------------------------\n",
    "\n",
    "# df_metadata[\"context_id\"] = range(len(df_metadata))\n",
    "# df_chunks[\"context_id\"] = range(len(df_chunks))\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# 6. Create embedding indexes\n",
    "# -------------------------\n",
    "\n",
    "metadata_index = embedding.EmbeddingIndex()\n",
    "metadata_index.add_embedding(\n",
    "    \"metadata_embedding\",\n",
    "    embedding_dim,\n",
    ")\n",
    "\n",
    "chunk_index = embedding.EmbeddingIndex()\n",
    "chunk_index.add_embedding(\n",
    "    \"chunk_embedding\",\n",
    "    embedding_dim,\n",
    ")\n",
    "\n",
    "print(\"Embedding indexes created.\")\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# 7. Final sanity check\n",
    "# -------------------------\n",
    "\n",
    "display(df_metadata.head())\n",
    "display(df_chunks.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2bced31",
   "metadata": {
    "id": "d2bced31"
   },
   "source": [
    "## <span style=\"color:#ff5f27;\"> üîÆ Connecting to Hopsworks Feature Store </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7caf764d",
   "metadata": {
    "id": "7caf764d"
   },
   "outputs": [],
   "source": [
    "import hopsworks\n",
    "from config import HOPSWORKS_API_KEY\n",
    "# project = hopsworks.login()\n",
    "\n",
    "project = hopsworks.login(\n",
    "        # project=HOPSWORKS_PROJECT,\n",
    "        api_key_value=HOPSWORKS_API_KEY\n",
    "    )\n",
    "\n",
    "fs = project.get_feature_store()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed9ac69",
   "metadata": {
    "id": "0ed9ac69"
   },
   "source": [
    "## <span style=\"color:#ff5f27;\"> ü™Ñ Feature Group Creation </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5e486b",
   "metadata": {
    "id": "9f5e486b"
   },
   "outputs": [],
   "source": [
    "# === Cell 6.1: Create or Get Metadata Feature Group (Safe Version) ===\n",
    "\n",
    "from hsfs import embedding\n",
    "\n",
    "metadata_emb_index = embedding.EmbeddingIndex()\n",
    "metadata_emb_index.add_embedding(\n",
    "    \"embedding\",\n",
    "    model.get_sentence_embedding_dimension(),\n",
    ")\n",
    "\n",
    "metadata_fg = fs.get_or_create_feature_group(\n",
    "    name=\"paper_metadata_fg_2\",\n",
    "    version=3,\n",
    "    description=\"New chunk splitting to avoid cut a word\",\n",
    "    primary_key=[\"paper_id\"],\n",
    "    online_enabled=True,\n",
    "    embedding_index=metadata_emb_index,\n",
    ")\n",
    "\n",
    "\n",
    "metadata_fg.insert(\n",
    "    df_metadata,\n",
    "    write_options={\"wait_for_job\": True},\n",
    ")\n",
    "\n",
    "print(f\"Inserted {len(df_metadata)} rows into metadata feature group.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e32b548",
   "metadata": {
    "id": "6e32b548"
   },
   "outputs": [],
   "source": [
    "# === Cell 6.2: Create or Get Fulltext Chunk Feature Group (Safe Version) ===\n",
    "\n",
    "chunk_emb_index = embedding.EmbeddingIndex()\n",
    "chunk_emb_index.add_embedding(\n",
    "    \"embedding\",\n",
    "    model.get_sentence_embedding_dimension(),\n",
    ")\n",
    "\n",
    "chunk_fg = fs.get_or_create_feature_group(\n",
    "    name=\"paper_chunk_fg_2\",\n",
    "    version=3,\n",
    "    description=\"New chunk splitting to avoid cut a word\",\n",
    "    primary_key=[\"paper_id\", \"chunk_index\"],\n",
    "    online_enabled=True,\n",
    "    embedding_index=chunk_emb_index,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2133dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_fg.insert(\n",
    "    df_chunks,\n",
    "    write_options={\"wait_for_job\": True},\n",
    ")\n",
    "\n",
    "print(f\"Inserted {len(df_chunks)} rows into chunk feature group.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39a9ed6",
   "metadata": {
    "id": "d39a9ed6"
   },
   "source": [
    "## <span style=\"color:#ff5f27;\">ü™Ñ Feature View Creation </span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "OJ6koRqW6KsN",
   "metadata": {
    "id": "OJ6koRqW6KsN"
   },
   "outputs": [],
   "source": [
    "# === Cell 8.1: Create Metadata Feature View ===\n",
    "\n",
    "metadata_fv = fs.get_or_create_feature_view(\n",
    "    name=\"paper_metadata_fv_2\",\n",
    "    version=3,\n",
    "    description=\"New chunk splitting to avoid cut a word\",\n",
    "    query=metadata_fg.select(\n",
    "        [\n",
    "            \"paper_id\",\n",
    "            \"title\",\n",
    "            \"abstract\",\n",
    "            \"authors\",\n",
    "            \"year\",\n",
    "            \"item_type\",\n",
    "            \"embedding\",\n",
    "        ]\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(\"Metadata Feature View ready.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Bj3WyVOX6MQS",
   "metadata": {
    "id": "Bj3WyVOX6MQS"
   },
   "outputs": [],
   "source": [
    "# === Cell 8.2: Create Chunk Feature View ===\n",
    "\n",
    "chunk_fv = fs.get_or_create_feature_view(\n",
    "    name=\"paper_chunk_fv_2\",\n",
    "    version=3,\n",
    "    description=\"New chunk splitting to avoid cut a word\",\n",
    "    query=chunk_fg.select(\n",
    "        [\n",
    "            \"paper_id\",\n",
    "            \"chunk_index\",\n",
    "            \"content\",\n",
    "            \"year\",\n",
    "            \"embedding\",\n",
    "        ]\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(\"Chunk Feature View ready.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708b9a5f",
   "metadata": {
    "id": "708b9a5f"
   },
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "https://github.com/alyssa2024/ID2223-Project/blob/main/1_feature_backfill.ipynb",
     "timestamp": 1767715450010
    }
   ]
  },
  "kernelspec": {
   "display_name": "aq",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
