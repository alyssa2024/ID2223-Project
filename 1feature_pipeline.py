import os
import pandas as pd
import hopsworks
from sentence_transformers import SentenceTransformer
from hsfs.embedding import EmbeddingIndex

# å¼•å…¥æ¨¡å—
from zotero_parser import ZoteroRDFParser
from pdf_extractor import ContentProcessor

# --- é…ç½® ---
PROJECT_NAME = "ä½ çš„Hopsworksé¡¹ç›®å"
RDF_PATH = "My Library.rdf"
BASE_DIR = "."  # é™„ä»¶æ–‡ä»¶å¤¹æ ¹ç›®å½•
MODEL_NAME = "all-MiniLM-L6-v2"

def main():
    # 1. åˆå§‹åŒ–è§£æå™¨
    parser = ZoteroRDFParser(RDF_PATH, BASE_DIR)
    papers = parser.parse()
    
    if not papers:
        print("No papers found to process.")
        return

    # å‡†å¤‡ä¸¤ä¸ªæ•°æ®åˆ—è¡¨ (åŒç´¢å¼•ç­–ç•¥)
    metadata_rows = []   # ç”¨äºå®½æ³›æœç´¢ (Title, Abstract)
    fulltext_rows = []   # ç”¨äºæ·±åº¦é˜…è¯» (Full Text Chunks)

    print("ğŸš€ Starting content extraction...")
    
    for paper in papers:
        # --- A. å¤„ç†æ­£æ–‡ (æå–å…¨æ–‡) ---
        full_text = ""
        # éå†è¯¥è®ºæ–‡çš„æ‰€æœ‰é™„ä»¶ï¼Œæ‰¾åˆ°ç¬¬ä¸€ä¸ªèƒ½è¯»å‡ºæ¥çš„
        for attach in paper["attachments"]:
            full_path = os.path.join(BASE_DIR, attach["path"])
            content = ContentProcessor.read_file(full_path, attach["type"])
            if len(content) > 100: # åªæœ‰å†…å®¹è¶³å¤Ÿæ‰ç®—æˆåŠŸ
                full_text = content
                break # åªè¦ä¸€ä»½æ­£æ–‡
        
        # å¦‚æœ Zotero æ²¡æ‘˜è¦ï¼Œå°è¯•ä»å…¨æ–‡è¡¥å…¨
        if not paper["abstract"] and full_text:
            fallback_abs = ContentProcessor.extract_abstract_fallback(full_text)
            if fallback_abs:
                paper["abstract"] = fallback_abs
                print(f"âœ¨ Extracted abstract for {paper['title'][:30]}...")

        # --- B. æ„å»º Metadata Row (Meta Index) ---
        # å³ä½¿æ²¡æœ‰æ­£æ–‡ï¼Œå…ƒæ•°æ®ä¹Ÿæ˜¯æœ‰ç”¨çš„
        metadata_rows.append({
            "paper_id": paper["id"],
            "title": paper["title"],
            "abstract": paper["abstract"],
            "authors": paper["authors"],
            "year": paper["year"],
            "category": paper["category"],
            # è¿™æ˜¯ç”¨äº Embedding çš„æ–‡æœ¬ï¼šåŒ…å«æ ‡é¢˜ã€æ‘˜è¦å’Œåˆ†ç±»
            "combined_text": f"Title: {paper['title']}\nCategory: {paper['category']}\nAbstract: {paper['abstract']}"
        })

        # --- C. æ„å»º Fulltext Rows (Content Index) ---
        if full_text:
            chunks = ContentProcessor.chunk_text(full_text)
            for i, chunk in enumerate(chunks):
                fulltext_rows.append({
                    "paper_id": paper["id"],
                    "chunk_index": i,
                    "content": chunk,
                    "year": paper["year"] # ä¿ç•™å¹´ä»½ç”¨äºè¿‡æ»¤
                })

    # 2. è¿æ¥ Hopsworks
    print(f"ğŸ’¾ Connecting to Hopsworks Project: {PROJECT_NAME}...")
    project = hopsworks.login(project=PROJECT_NAME)
    fs = project.get_feature_store()
    
    # åŠ è½½ Embedding æ¨¡å‹
    model = SentenceTransformer(MODEL_NAME)

    # --- 3. ä¸Šä¼  Metadata Feature Group ---
    if metadata_rows:
        print(f"Processing {len(metadata_rows)} metadata records...")
        df_meta = pd.DataFrame(metadata_rows)
        # ç”Ÿæˆå‘é‡
        df_meta['embedding'] = df_meta['combined_text'].apply(lambda x: model.encode(x).tolist())
        
        meta_fg = fs.get_or_create_feature_group(
            name="zotero_meta_fg",
            version=1,
            description="Paper Metadata (Abstracts) for Broad Search",
            primary_key=["paper_id"],
            online_enabled=True,
            embedding_index=EmbeddingIndex()
        )
        meta_fg.insert(df_meta)
        print("âœ… Metadata Feature Group uploaded.")

    # --- 4. ä¸Šä¼  Full-Text Feature Group ---
    if fulltext_rows:
        print(f"Processing {len(fulltext_rows)} full-text chunks...")
        df_text = pd.DataFrame(fulltext_rows)
        # ç”Ÿæˆå‘é‡
        df_text['embedding'] = df_text['content'].apply(lambda x: model.encode(x).tolist())
        
        text_fg = fs.get_or_create_feature_group(
            name="zotero_fulltext_fg",
            version=1,
            description="Full Text Chunks for Deep Reading",
            primary_key=["paper_id", "chunk_index"],
            online_enabled=True,
            embedding_index=EmbeddingIndex()
        )
        text_fg.insert(df_text)
        print("âœ… Full-Text Feature Group uploaded.")

if __name__ == "__main__":
    main()
